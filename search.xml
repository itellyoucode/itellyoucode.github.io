<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ElasticSearch核心知识学习</title>
    <url>/post/24e8e8ba/</url>
    <content><![CDATA[<h1 id="概念理解"><a href="#概念理解" class="headerlink" title="概念理解:"></a>概念理解:</h1><h2 id="什么是搜索"><a href="#什么是搜索" class="headerlink" title="什么是搜索?"></a>什么是搜索?</h2><p>百度、谷歌 != 搜索</p>
<p>垂直搜索（站内搜索）<br>互联网的搜索：电商网站，新闻网站，各种app<br>IT系统的搜索：员工搜索,项目管理等<br>搜索，就是在任意的场景通过一些关键信息,找出对应的数据的操作。</p>
<h2 id="数据库做搜索是怎样做的"><a href="#数据库做搜索是怎样做的" class="headerlink" title="数据库做搜索是怎样做的?"></a>数据库做搜索是怎样做的?</h2><p><img data-src="/images/loading.png" data-original="/images/pasted-15.png" alt="数据库搜索"></p>
<h2 id="全文检索和倒排索引是什么"><a href="#全文检索和倒排索引是什么" class="headerlink" title="全文检索和倒排索引是什么?"></a>全文检索和倒排索引是什么?</h2><p><img data-src="/images/loading.png" data-original="/images/pasted-14.png" alt="倒排索引和全文检索"></p>
<h2 id="什么是ElasticSearch"><a href="#什么是ElasticSearch" class="headerlink" title="什么是ElasticSearch?"></a>什么是ElasticSearch?</h2><ol>
<li>基于lucene，隐藏复杂性，提供简单易用的restful api接口、java api接口（还有其他语言的api接口）</li>
<li>开箱即用，优秀的默认参数，不需要任何额外设置，完全开源</li>
<li>Elasticsearch，分布式，高性能，高可用，可伸缩的搜索和分析系统</li>
</ol>
<p><img data-src="/images/loading.png" data-original="/images/pasted-13.png" alt="架构"></p>
<h3 id="elasticsearch的核心概念"><a href="#elasticsearch的核心概念" class="headerlink" title="elasticsearch的核心概念"></a>elasticsearch的核心概念</h3><ol>
<li>Near Realtime（NRT）：近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级</li>
<li>cluster：代表一个集群，集群中有多个节点，其中有一个为主节点，这个主节点是可以通过选举产生的，主从节点是对于集群内部来说的。es的一个概念就是去中心化，字面上理解就是无中心节点，这是对于集群外部来说的，因为从外部来看es集群，在逻辑上是个整体，你与任何一个节点的通信和与整个es集群通信是等价的。</li>
<li>shards：代表索引分片，es可以把一个完整的索引分成多个分片，这样的好处是可以把一个大的索引拆分成多个，分布到不同的节点上。构成分布式搜索。分片的数量只能在索引创建前指定，并且索引创建后不能更改。</li>
<li>replicas：代表索引副本，es可以设置多个索引的副本，副本的作用一是提高系统的容错性，当某个节点某个分片损坏或丢失时可以从副本中恢复。二是提高es的查询效率，es会自动对搜索请求进行负载均衡。</li>
<li>recovery：代表数据恢复或叫数据重新分布，es在有节点加入或退出时会根据机器的负载对索引分片进行重新分配，挂掉的节点重新启动时也会进行数据恢复。</li>
<li>river：代表es的一个数据源，也是其它存储方式（如：数据库）同步数据到es的一个方法。它是以插件方式存在的一个es服务，通过读取river中的数据并把它索引到es中，官方的river有couchDB的，RabbitMQ的，Twitter的，Wikipedia的。</li>
<li>gateway：代表es索引快照的存储方式，es默认是先把索引存放到内存中，当内存满了时再持久化到本地硬盘。gateway对索引快照进行存储，当这个es集群关闭再重新启动时就会从gateway中读取索引备份数据。es支持多种类型的gateway，有本地文件系统（默认），分布式文件系统，Hadoop的HDFS和amazon的s3云存储服务。</li>
<li>discovery.zen：代表es的自动发现节点机制，es是一个基于p2p的系统，它先通过广播寻找存在的节点，再通过多播协议来进行节点之间的通信，同时也支持点对点的交互。</li>
<li>Transport：代表es内部节点或集群与客户端的交互方式，默认内部是使用tcp协议进行交互，同时它支持http协议（json格式）、thrift、servlet、memcached、zeroMQ等的传输协议（通过插件方式集成）。</li>
</ol>
<h3 id="elasticsearch核心概念-vs-数据库核心概念"><a href="#elasticsearch核心概念-vs-数据库核心概念" class="headerlink" title="elasticsearch核心概念 vs. 数据库核心概念"></a>elasticsearch核心概念 vs. 数据库核心概念</h3><table>
<thead>
<tr>
<th>Elasticsearch</th>
<th>数据库</th>
</tr>
</thead>
<tbody><tr>
<td>Document</td>
<td>行</td>
</tr>
<tr>
<td>Type</td>
<td>表</td>
</tr>
<tr>
<td>Index</td>
<td>库</td>
</tr>
</tbody></table>
<ul>
<li>Index：索引，包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document。</li>
<li>Type：类型，<del>每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户数据type，博客数据type，评论数据type。</del> ==6.0版本兼容多Type,7.0后一个Index只支持一个Type==</li>
<li>Document&amp;field：文档，es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。一个document里面有多个field，每个field就是一个数据字段。</li>
</ul>
<h2 id="集群详解"><a href="#集群详解" class="headerlink" title="集群详解"></a>集群详解</h2><p><img data-src="/images/loading.png" data-original="/images/pasted-12.png" alt="集群结构"></p>
<h3 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h3><p><img data-src="/images/loading.png" data-original="/images/pasted-18.png" alt="基础架构"></p>
<ul>
<li>复杂的分布式机制,比如:分片、副本、负载均衡、路由等等都被隐藏起来。</li>
<li>Elasticsearch的垂直扩容与水平扩容<ul>
<li>垂直扩容：购买更好的服务</li>
<li>水平扩容：购买更多普通服务加入集群</li>
</ul>
</li>
<li>增减或减少节点时的数据rebalance,尽量保持负载均衡</li>
<li>master节点概念<ul>
<li>创建或删除索引</li>
<li>增加或删除节点</li>
</ul>
</li>
<li>节点平等的分布式架构<ul>
<li>节点对等，每个节点都能接收所有的请求</li>
<li>自动请求路由</li>
<li>响应收集</li>
</ul>
</li>
</ul>
<h3 id="容错机制：master选举，replica容错，数据恢复"><a href="#容错机制：master选举，replica容错，数据恢复" class="headerlink" title="容错机制：master选举，replica容错，数据恢复"></a>容错机制：master选举，replica容错，数据恢复</h3><p><img data-src="/images/loading.png" data-original="/images/pasted-17.png" alt="容错机制"></p>
<ol>
<li>9 shard，3 node</li>
<li>master node宕机，自动master选举，集群状态:red</li>
<li>replica容错：新master将replica提升为primary shard，集群状态:yellow</li>
<li>重启宕机node，master copy replica到该node，使用原有的shard并同步宕机后的修改，集群状态:green</li>
</ol>
<h3 id="路由机制"><a href="#路由机制" class="headerlink" title="路由机制"></a>路由机制</h3><p><img data-src="/images/loading.png" data-original="/images/pasted-19.png" alt="路由机制"></p>
<ul>
<li>路由算法:<code>shard_num = hash(_routing) % num_primary_shards</code><ul>
<li>其中 _routing 是一个可变值，默认是文档的 _id 的值 ，也可以设置成一个自定义的值。 _routing 通过 hash 函数生成一个数字，然后这个数字再除以 num_of_primary_shards （主分片的数量）后得到余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。</li>
</ul>
</li>
<li>假设你有一个100个分片的索引。当一个请求在集群上执行时会发生什么呢？<ol>
<li>这个搜索的请求会被发送到一个节点</li>
<li>接收到这个请求的节点，将这个查询广播到这个索引的每个分片上（可能是主分片，也可能是复本分片）</li>
<li>每个分片执行这个搜索查询并返回结果</li>
<li>结果在通道节点上合并、排序并返回给用户</li>
</ol>
</li>
<li>可自定义路由,例如将相同的商户(merchant_id)数据路由到同一个shard</li>
</ul>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>elsticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>面试经</title>
    <url>/post/4048efa0/</url>
    <content><![CDATA[<h2 id="java基础："><a href="#java基础：" class="headerlink" title="java基础："></a>java基础：</h2><h3 id="1-HashMap的数据结构是什么？"><a href="#1-HashMap的数据结构是什么？" class="headerlink" title="1.HashMap的数据结构是什么？"></a>1.HashMap的数据结构是什么？</h3><p>JDK版本 实现方式 节点数&gt;=8 节点数&lt;=6<br>1.8以前 数组+单向链表 数组+单向链表 数组+单向链表<br>1.8以后 数组+单向链表+红黑树 数组+红黑树 数组+单向链表<br>寻址算法优化<br>(n - 1) &amp; hash -&gt; 数组里的一个位置</p>
<h3 id="2、HashSet-是如何保证不重复的"><a href="#2、HashSet-是如何保证不重复的" class="headerlink" title="2、HashSet 是如何保证不重复的"></a>2、HashSet 是如何保证不重复的</h3><p>底层维护了一个HashMap</p>
<h3 id="3、HashMap-是线程安全的吗，为什么不是线程安全的（最好画图说明多线程环境下不安全）"><a href="#3、HashMap-是线程安全的吗，为什么不是线程安全的（最好画图说明多线程环境下不安全）" class="headerlink" title="3、HashMap 是线程安全的吗，为什么不是线程安全的（最好画图说明多线程环境下不安全）?"></a>3、HashMap 是线程安全的吗，为什么不是线程安全的（最好画图说明多线程环境下不安全）?</h3><p>线程不安全</p>
<h3 id="4、HashMap-的扩容过程"><a href="#4、HashMap-的扩容过程" class="headerlink" title="4、HashMap 的扩容过程"></a>4、HashMap 的扩容过程</h3><p>初始长度是16，并且每次扩展时候，长度必须是2的幂。因为2的幂-1对应的二进制末尾总是是1111，这样，和key.hash做“与”运算的时候，只要输入的HashCode本身分布均匀，Hash算法的结果就是均匀的。</p>
<h3 id="5、HashMap-1-7-与-1-8-的-区别，说明-1-8-做了哪些优化，如何优化的？"><a href="#5、HashMap-1-7-与-1-8-的-区别，说明-1-8-做了哪些优化，如何优化的？" class="headerlink" title="5、HashMap 1.7 与 1.8 的 区别，说明 1.8 做了哪些优化，如何优化的？"></a>5、HashMap 1.7 与 1.8 的 区别，说明 1.8 做了哪些优化，如何优化的？</h3><p>JDK版本 实现方式 节点数&gt;=8 节点数&lt;=6<br>1.8以前 数组+单向链表 数组+单向链表 数组+单向链表<br>1.8以后 数组+单向链表+红黑树 数组+红黑树 数组+单向链表<br>寻址算法优化<br>(n - 1) &amp; hash -&gt; 数组里的一个位置</p>
<h3 id="6、final-finally-finalize"><a href="#6、final-finally-finalize" class="headerlink" title="6、final finally finalize"></a>6、final finally finalize</h3><p>final用于声明属性，方法和类，分别表示属性不可交变，方法不可覆盖，类不可继承。<br>finally是异常处理语句结构的一部分，表示总是执行。<br>finalize是Object类的一个方法，在垃圾收集器执行的时候会调用被回收对象的此方法，供垃圾收集时的其他资源回收，例如关闭文件等。</p>
<h3 id="7、强引用-、软引用、-弱引用、虚引用"><a href="#7、强引用-、软引用、-弱引用、虚引用" class="headerlink" title="7、强引用 、软引用、 弱引用、虚引用"></a>7、强引用 、软引用、 弱引用、虚引用</h3><p>强引用（Strong Reference）：通常我们通过new来创建一个新对象时返回的引用就是一个强引用，若一个对象通过一系列强引用可到达，它就是强可达的(strongly reachable)，那么它就不被回收；<br>软引用（Soft Reference）：软引用和弱引用的区别在于，若一个对象是弱引用可达，无论当前内存是否充足它都会被回收，而软引用可达的对象在内存不充足时才会被回收，因此软引用要比弱引用“强”一些；<br>弱引用（Weak Reference）：弱引用简单来说就是将对象留在内存的能力不是那么强的引用。使用Weak Reference，垃圾回收器会帮你来决定引用的对象何时回收并且将对象从内存移除。ThreadLocal的key使用了弱引用。<br>虚引用（Phantom Reference）：虚引用是Java中最弱的引用，那么它弱到什么程度呢？它是如此脆弱以至于我们通过虚引用甚至无法获取到被引用的对象，虚引用存在的唯一作用就是当它指向的对象被回收后，虚引用本身会被加入到引用队列中，用作记录它指向的对象已被回收。</p>
<h3 id="8、Java反射的实现原理"><a href="#8、Java反射的实现原理" class="headerlink" title="8、Java反射的实现原理"></a>8、Java反射的实现原理</h3><p>反射:(reflection):在运行时期,动态地去获取类中的信息(类的信息,方法信息,构造器信息,字段等信息进行操作);<br>一个类中包含的信息有： 构造器，字段，方法<br>如何使用反射描述这些相关的信息<br>Class ： 描述类<br>Method ： 描述方法<br>Constructor ：描述构造器<br>Field ：描述字段</p>
<h3 id="9、Arrays-sort-实现原理和-Collections-实现原理"><a href="#9、Arrays-sort-实现原理和-Collections-实现原理" class="headerlink" title="9、Arrays.sort 实现原理和 Collections 实现原理"></a>9、Arrays.sort 实现原理和 Collections 实现原理</h3><p>事实上Collections.sort方法底层就是调用的array.sort方法，而且不论是Collections.sort或者是Arrays.sort方法，<br>不论是Collections.sort方法或者是Arrays.sort方法，底层实现都是TimSort实现的，这是jdk1.7新增的，以前是归并排序。TimSort算法就是找到已经排好序数据的子序列，然后对剩余部分排序，然后合并起来</p>
<h3 id="10、LinkedHashMap的应用"><a href="#10、LinkedHashMap的应用" class="headerlink" title="10、LinkedHashMap的应用"></a>10、LinkedHashMap的应用</h3><p>LinkedHashMap是HashMap的子类，但是内部还有一个双向链表维护键值对的顺序，每个键值对既位于哈希表中，也位于双向链表中。LinkedHashMap支持两种顺序插入顺序 、 访问顺序</p>
<p>插入顺序：先添加的在前面，后添加的在后面。修改操作不影响顺序<br>访问顺序：所谓访问指的是get/put操作，对一个键执行get/put操作后，其对应的键值对会移动到链表末尾，所以最末尾的是最近访问的，最开始的是最久没有被访问的，这就是访问顺序。</p>
<p>可以设置访问顺序用做JVM LRU缓存</p>
<h3 id="11、cloneable接口实现原理"><a href="#11、cloneable接口实现原理" class="headerlink" title="11、cloneable接口实现原理"></a>11、cloneable接口实现原理</h3><p>Cloneable是一个标记接口，里面没有任何的方法。<br>java的一个类，如果要使用Cloneable实现拷贝功能，需要先实现这个接口，然后重写Object的clone方法。对于类中的引用类型的属性，需要在clone方法中实现深拷贝，否则就是浅拷贝。</p>
<h3 id="12、异常分类以及处理机制"><a href="#12、异常分类以及处理机制" class="headerlink" title="12、异常分类以及处理机制"></a>12、异常分类以及处理机制</h3><p>异常的分类：<br>　Throwable是根接口。<br>　Error和Exception继承这个接口 。<br>　　Error是无法处理的异常，比如OutOfMemoryError，一般发生这种异常，JVM会选择终止程序。因此我们编写程序时不需要关心这类异常。<br>　　Exception，也就是我们经常见到的一些异常情况，这些异常是我们可以处理的异常，是所有异常类的父类。<br>　　RuntimeException和checked exception(受査异常也称非运行时异常)继承Exception<br>　　 unchecked Exception(非受査异常)，包括Error和RuntimeException，比如常见的NullPointerException、IndexOutOfBoundsException。对于RuntimeException，java编译器不要求必须进行异常捕获处理或者抛出 声明，由程序员自行决定。<br>　　checked exception（受查异常），也称非运行时异常（运行时异常以外的异常就是非运行时异常），由代 码能力之外的因素导致的运行时错误。java编译器强制程序员必须进行捕获处理，比如常见的有IOExeption 和SQLException。如果不进行捕获或者抛出声明处理，编译都不会通过。<br>处理机制:<br>捕获机制：try-catch-finally<br>try-监控区域，执行可能产生异常的代码<br>catch-捕获，处理异常<br>finally-善后处理，无论是否发生异常，代码总能执行<br>抛出异常：throw 手动抛出异常<br>声明异常：throws 声明方法可能要抛出的异常<br>throw：手动抛出异常，一般由程序员在方法内抛出Exception的子类异常。<br>throws：声明在方法名之后，告诉调用者，该方法可能会抛出异常，也就是说异常发生后会抛给调用者，由 调用者处理异常。</p>
<p>请写出5种常见到的runtime exception<br>常见的几种如下：<br>NullPointerException - 空指针引用异常<br>ClassCastException - 类型强制转换异常。<br>IllegalArgumentException - 传递非法参数异常。<br>ArithmeticException - 算术运算异常<br>ArrayStoreException - 向数组中存放与声明类型不兼容对象异常<br>IndexOutOfBoundsException - 下标越界异常<br>NegativeArraySizeException - 创建一个大小为负数的数组错误异常<br>NumberFormatException - 数字格式异常<br>SecurityException - 安全异常<br>UnsupportedOperationException - 不支持的操作异常</p>
<h3 id="13、数组在内存中如何分配"><a href="#13、数组在内存中如何分配" class="headerlink" title="13、数组在内存中如何分配"></a>13、数组在内存中如何分配</h3><h3 id="14-io的模型和nio-selectionkey是什么"><a href="#14-io的模型和nio-selectionkey是什么" class="headerlink" title="14.io的模型和nio selectionkey是什么"></a>14.io的模型和nio selectionkey是什么</h3><h3 id="ConcurrentHashMap怎么实现高并发的"><a href="#ConcurrentHashMap怎么实现高并发的" class="headerlink" title="ConcurrentHashMap怎么实现高并发的"></a>ConcurrentHashMap怎么实现高并发的</h3><p>ConcurrentHashMap底层采用分段的数组+链表实现，线程安全(读操作不加锁，由于HashEntry的value变量是 volatile的，也能保证读取到最新的值。)<br>ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术<br>有些方法需要跨段，比如size()和containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要按顺序锁定所有段，操作完毕后，又按顺序释放所有段的锁</p>
<h2 id="多线程："><a href="#多线程：" class="headerlink" title="多线程："></a>多线程：</h2><h3 id="1-什么是多线程，多线程的目的是什么？"><a href="#1-什么是多线程，多线程的目的是什么？" class="headerlink" title="1.什么是多线程，多线程的目的是什么？"></a>1.什么是多线程，多线程的目的是什么？</h3><p>1、避免阻塞（异步调用）<br>2、避免CPU空转<br>3、提升性能</p>
<h3 id="2-什么是线程安全，非线程安全？"><a href="#2-什么是线程安全，非线程安全？" class="headerlink" title="2.什么是线程安全，非线程安全？"></a>2.什么是线程安全，非线程安全？</h3><p>在拥有共享数据的多条线程并行执行的程序中，线程安全的代码会通过同步机制保证各个线程都可以正常且正确的执行，不会出现数据污染等意外情况。</p>
<h3 id="3-线程的有那几个状态以及相互间的转换"><a href="#3-线程的有那几个状态以及相互间的转换" class="headerlink" title="3.线程的有那几个状态以及相互间的转换"></a>3.线程的有那几个状态以及相互间的转换</h3><p>创建<br>一般就是创建一个方法继承thread或实现runable方法的类，通过new来创建了。<br>就绪<br>当一个线程调用了start方法后，还没有获取到cpu线程。<br>运行<br>当就绪状态的线程获取到cpu后，便开始运行。<br>阻塞<br>当运行时的线程调用wait、sleep、join等方法后便进入阻塞状态，当线程重新被唤醒时，就会进入就绪或运行。<br>死亡<br>当线程的run方法执行完成后，或调用stop方法后就会死亡。</p>
<h3 id="4-wait-notify为什么必须存在于synchronized块中？"><a href="#4-wait-notify为什么必须存在于synchronized块中？" class="headerlink" title="4.wait/notify为什么必须存在于synchronized块中？"></a>4.wait/notify为什么必须存在于synchronized块中？</h3><p>也就是说wait/notify是线程之间的通信，他们存在竞态，我们必须保证在满足条件的情况下才进行wait。换句话说，如果不加锁的话，那么wait被调用的时候可能wait的条件已经不满足了(如上述)。由于错误的条件下进行了wait，那么就有可能永远不会被notify到，所以我们需要强制wait/notify在synchronized中</p>
<h3 id="5-多线程volatile关键字的作用？"><a href="#5-多线程volatile关键字的作用？" class="headerlink" title="5.多线程volatile关键字的作用？"></a>5.多线程volatile关键字的作用？</h3><p>volatile——Java虚拟机提供的最轻量级的同步机制。这个关键字的作用就是告诉编译器，只要是被此关键字修饰的变量都是易变的、不稳定的。<br>        volatile特性之一：<br>  保证变量在线程之间的可见性。可见性的保证是基于CPU的内存屏障指令，被JSR-133抽象为happens-before原则。<br>        volatile特性之二：<br>  阻止编译时和运行时的指令重排。编译时JVM编译器遵循内存屏障的约束，运行时依靠CPU屏障指令来阻止重排。</p>
<h3 id="6-Runnable，Thread，Callable三者的区别？"><a href="#6-Runnable，Thread，Callable三者的区别？" class="headerlink" title="6.Runnable，Thread，Callable三者的区别？"></a>6.Runnable，Thread，Callable三者的区别？</h3><p>Java中实现多线程有3种方法：<br>继承Thread类<br>实现Runnable接口<br>实现Callable接口(参考&lt;Java编程思想(第4版)&gt;  21.2.4章节，原来一直以为是2种，后来发现是3种)</p>
<p>Thread是继承重写run方法,可以直接执行start方法启动线程<br>Runnable是实现接口,本身不带线程启动方法,需要丢到Thread或者线程池里执行<br>Runnable是执行工作的独立任务，但是它不返回任何值。如果你希望任务在完成的能返回一个值，那么可以实现Callable接口而不是Runnable接口。在Java SE5中引入的Callable是一种具有类型参数的泛型，它的参数类型表示的是从方法call()(不是run())中返回的值。<br>总结<br>实现Runnable接口相比继承Thread类有如下优势：</p>
<p>可以避免由于Java的单继承特性而带来的局限；<br>增强程序的健壮性，代码能够被多个线程共享，代码与数据是独立的；<br>适合多个相同程序代码的线程区处理同一资源的情况。</p>
<p>实现Runnable接口和实现Callable接口的区别:<br>Runnable是自从java1.1就有了，而Callable是1.5之后才加上去的<br>Callable规定的方法是call(),Runnable规定的方法是run()<br>Callable的任务执行后可返回值，而Runnable的任务是不能返回值(是void)<br>call方法可以抛出异常，run方法不可以<br>运行Callable任务可以拿到一个Future对象，表示异步计算的结果。它提供了检查计算是否完成的方法，以等待计算的完成，并检索计算的结果。通过Future对象可以了解任务执行情况，可取消任务的执行，还可获取执行结果。<br>加入线程池运行，Runnable使用ExecutorService的execute方法，Callable使用submit方法。</p>
<h3 id="7-ThreadLocal-的作用，怎么使用它？"><a href="#7-ThreadLocal-的作用，怎么使用它？" class="headerlink" title="7.ThreadLocal 的作用，怎么使用它？"></a>7.ThreadLocal 的作用，怎么使用它？</h3><p>在每个线程中存储一个变量的副本，这样在每个线程对该变量进行使用的使用，使用的即是该线程的局部变量，从而保证了线程的安全性以及高效性。</p>
<p>ThreadLocal的使用场景：<br>在并发编程中时常有这样一种需求：每条线程都需要存取一个同名变量，但每条线程中该变量的值均不相同。比如后台系统登录账号信息</p>
<h3 id="8-Lock与synchronized的区别？"><a href="#8-Lock与synchronized的区别？" class="headerlink" title="8.Lock与synchronized的区别？"></a>8.Lock与synchronized的区别？</h3><p>1.　　Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现，synchronized是在JVM层面上实现的，不但可以通过一些监控工具监控synchronized的锁定，而且在代码执行时出现异常，JVM会自动释放锁定。<br>　　   但是使用Lock则不行，lock是通过代码实现的，要保证锁定一定会被释放，就必须将 unLock()放到finally{} 中；<br>2.　　synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；<br>　　　而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁；<br>3.　　Lock可以让等待锁的线程响应中断，线程可以中断去干别的事务，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断<br>4.　　通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。<br>5.　　Lock可以提高多个线程进行读操作的效率。</p>
<h3 id="9-请对比hashTable-concurrentHashMap-HashMap的区别？"><a href="#9-请对比hashTable-concurrentHashMap-HashMap的区别？" class="headerlink" title="9.请对比hashTable , concurrentHashMap, HashMap的区别？"></a>9.请对比hashTable , concurrentHashMap, HashMap的区别？</h3><p>HashMap是Hashtable的轻量级实现（非线程安全的实现），他们都完成了Map接口，主要区别在于HashMap允许空（null）键值（key）,由于非线程安全，在只有一个线程访问的情况下，效率要高于Hashtable。<br>HashMap允许将null作为一个entry的key或者value，而Hashtable不允许。<br>HashMap把Hashtable的contains方法去掉了，改成containsvalue和containsKey。因为contains方法容易让人引起误解。<br>Hashtable继承自Dictionary类，而HashMap是Java1.2引进的Map interface的一个实现。<br>最大的不同是，Hashtable的方法是Synchronize的，而HashMap不是，在多个线程访问Hashtable时，不需要自己为它的方法实现同步，而HashMap 就必须为之提供外同步。</p>
<h3 id="10-什么是线程死锁？产生的原因？如何避免死锁？"><a href="#10-什么是线程死锁？产生的原因？如何避免死锁？" class="headerlink" title="10.什么是线程死锁？产生的原因？如何避免死锁？"></a>10.什么是线程死锁？产生的原因？如何避免死锁？</h3><p>产生死锁必须同时满足以下四个条件，只要其中任一条件不成立，死锁就不会发生。</p>
<p>互斥条件：进程要求对所分配的资源（如打印机）进行排他性控制，即在一段时间内某 资源仅为一个进程所占有。此时若有其他进程请求该资源，则请求进程只能等待。<br>不剥夺条件：进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能 由获得该资源的进程自己来释放（只能是主动释放)。<br>请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源 已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。<br>循环等待条件：存在一种进程资源的循环等待链，链中每一个进程已获得的资源同时被 链中下一个进程所请求。即存在一个处于等待状态的进程集合{Pl, P2, …, pn}，其中Pi等 待的资源被P(i+1)占有（i=0, 1, …, n-1)，Pn等待的资源被P0占有，如图2-15所示。</p>
<p>在有些情况下死锁是可以避免的。三种用于避免死锁的技术：</p>
<p>加锁顺序（线程按照一定的顺序加锁）<br>加锁时限（线程尝试获取锁的时候加上一定的时限，超过时限则放弃对该锁的请求，并释放自己占有的锁）<br>死锁检测</p>
<h3 id="11-什么是Executors框架？"><a href="#11-什么是Executors框架？" class="headerlink" title="11.什么是Executors框架？"></a>11.什么是Executors框架？</h3><h3 id="12-什么是Callable和Future"><a href="#12-什么是Callable和Future" class="headerlink" title="12.什么是Callable和Future?"></a>12.什么是Callable和Future?</h3><h3 id="13-在静态方法上使用同步时会发生什么事？"><a href="#13-在静态方法上使用同步时会发生什么事？" class="headerlink" title="13.在静态方法上使用同步时会发生什么事？"></a>13.在静态方法上使用同步时会发生什么事？</h3><p>同步静态方法时会获取该类的“Class”对象，所以当一个线程进入同步的静态方法中时，线程监视器获取类本身的对象锁，其它线程不能进入这个类的任何静态同步方法。它不像实例方法，因为多个线程可以同时访问不同实例同步实例方法。</p>
<h3 id="14-什么是线程饿死，什么是活锁？"><a href="#14-什么是线程饿死，什么是活锁？" class="headerlink" title="14.什么是线程饿死，什么是活锁？"></a>14.什么是线程饿死，什么是活锁？</h3><p>活锁：线程A和B都需要过桥(都需要使用进程),而都礼让不走(那到的系统优先级相同,都认为不是自己优先级高),就这么僵持下去.（很绅士，互相谦让）</p>
<p>并未产生线程阻塞，但是由于某种问题的存在，导致无法继续执行的情况。</p>
<p>消息重试。当某个消息处理失败的时候，一直重试，但重试由于某种原因，比如消息格式不对，导致解析失败，而它又被重试</p>
<p>这种时候一般是将不可修复的错误不要重试，或者是重试次数限定<br>相互协作的线程彼此响应从而修改自己状态，导致无法执行下去。比如两个很有礼貌的人在同一条路上相遇，彼此给对方让路，但是又在同一条路上遇到了。互相之间反复的避让下去</p>
<p>这种时候可以选择一个随机退让，使得具备一定的随机性</p>
<p>饥饿：:这是个独木桥(单进程),桥上只能走一个人,B来到时A在桥上,B等待;<br>        而此时比B年龄小的C来了,B让C现行(A走完后系统把进程分给了C),<br>        C上桥后,D又来了,B又让D现行(C走完后系统把进程分个了D)<br>        以此类推B一直是等待状态.</p>
<h2 id="线程池："><a href="#线程池：" class="headerlink" title="线程池："></a>线程池：</h2><h3 id="1-解释线程池的作用？"><a href="#1-解释线程池的作用？" class="headerlink" title="1.解释线程池的作用？"></a>1.解释线程池的作用？</h3><p>1）降低资源消耗；2）提高响应速度；3）提高线程的可管理性。<br>线程的创建和销毁的开销是巨大的，而通过线程池的重用大大减少了这些不必要的开销，当然既然少了这么多消费内存的开销，其线程执行速度也是突飞猛进的提升。</p>
<p>并发：在某个时间段内，多个程序都处在执行和执行完毕之间；但在一个时间点上只有一个程序在运行。头脑风暴：老鹰妈妈喂小雏鹰食物，小雏鹰很多，而老鹰只有一张嘴，她需要一个个喂过去，到最后每个小雏鹰都可以吃到，但是在一个时间点里只能有一个小雏鹰可以吃到美味的食物。</p>
<p>并行：在某个时间段里，每个程序按照自己独立异步的速度执行，程序之间互不干扰。头脑风暴：这就好似老鹰妈妈决定这样喂食太费劲于是为每个小雏鹰请了个保姆，这样子在一个时间点里，每个小雏鹰都可以同时吃到食物，而且互相不干扰。</p>
<h3 id="2-线程池的处理流程？"><a href="#2-线程池的处理流程？" class="headerlink" title="2.线程池的处理流程？"></a>2.线程池的处理流程？</h3><p><img data-src="/images/loading.png" data-original="/images/pasted-11.png" alt="upload successful"><br>线程池优先要创建出基本线程池大小（corePoolSize）的线程数量，没有达到这个数量时，每次提交新任务都会直接创建一个新线程，当达到了基本线程数量后，又有新任务到达，优先放入等待队列，如果队列满了，才去创建新的线程（不能超过线程池的最大数maxmumPoolSize）</p>
<h3 id="3-jdk提供的线程池工具类有哪些，区别是什么？"><a href="#3-jdk提供的线程池工具类有哪些，区别是什么？" class="headerlink" title="3.jdk提供的线程池工具类有哪些，区别是什么？"></a>3.jdk提供的线程池工具类有哪些，区别是什么？</h3><p>newFixedThreadPool<br>创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。</p>
<p>newCachedThreadPool<br>创建一个可缓存的线程池。这种类型的线程池特点是：<br>1).工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger. MAX_VALUE), 这样可灵活的往线程池中添加线程。<br>2).如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间(默认为1分钟)，则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。</p>
<p>newSingleThreadExecutor<br>创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，如果这个线程异常结束，会有另一个取代它，保证顺序执行(我觉得这点是它的特色)。单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的 。</p>
<p>newScheduleThreadPool<br>创建一个定长的线程池，而且支持定时的以及周期性的任务执行，类似于Timer。(这种线程池原理暂还没完全了解透彻)</p>
<h3 id="4-关闭线程池的方法有哪些？区别是什么？"><a href="#4-关闭线程池的方法有哪些？区别是什么？" class="headerlink" title="4.关闭线程池的方法有哪些？区别是什么？"></a>4.关闭线程池的方法有哪些？区别是什么？</h3><p>一种是调用其shutdown()方法，另一种是调用shutdownNow()方法<br>shutdown:<br>1、调用之后不允许继续往线程池内继续添加线程;<br>2、线程池的状态变为SHUTDOWN状态;<br>3、所有在调用shutdown()方法之前提交到ExecutorSrvice的任务都会执行;<br>4、一旦所有线程结束执行当前任务，ExecutorService才会真正关闭。</p>
<p>shutdownNow():<br>1、该方法返回尚未执行的 task 的 List;<br>2、线程池的状态变为STOP状态;<br>3、阻止所有正在等待启动的任务, 并且停止当前正在执行的任务。</p>
<p>简单点来说，就是:<br>shutdown()调用后，不可以再 submit 新的 task，已经 submit 的将继续执行<br>shutdownNow()调用后，试图停止当前正在执行的 task，并返回尚未执行的 task 的 list</p>
<h2 id="MySqL："><a href="#MySqL：" class="headerlink" title="MySqL："></a>MySqL：</h2><h3 id="1-sql优化方法？"><a href="#1-sql优化方法？" class="headerlink" title="1.sql优化方法？"></a>1.sql优化方法？</h3><p>合理建立索引;对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引<br>应尽量避免使用!=和&lt;&gt;条件判断进行全表扫描;<br>应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描;<br>应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描;<br>in 和 not in 也要慎用，否则会导致全表扫描<br>避免模糊查询进行全表扫描<br>避免在字段上做表达式操作</p>
<h3 id="2-建索引有哪些策略和原则？"><a href="#2-建索引有哪些策略和原则？" class="headerlink" title="2.建索引有哪些策略和原则？"></a>2.建索引有哪些策略和原则？</h3><p>1.最左前缀匹配原则<br>非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。</p>
<p>2.=和in可以乱序<br>比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式</p>
<p>3.尽量选择区分度高的列作为索引<br>区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录</p>
<p>4.索引列不能参与计算<br>保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’);</p>
<p>5.尽量的扩展索引，不要新建索引。<br>比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可</p>
<p>6．选择唯一性索引<br>唯一性索引的值是唯一的，可以更快速的通过该索引来确定某条记录。例如，学生表中学号是具有唯一性的字段。为该字段建立唯一性索引可以很快的确定某个学生的信息。如果使用姓名的话，可能存在同名现象，从而降低查询速度。</p>
<p>7．为经常需要排序、分组和联合操作的字段建立索引<br>经常需要ORDER BY、GROUP BY、DISTINCT和UNION等操作的字段，排序操作会浪费很多时间。如果为其建立索引，可以有效地避免排序操作。</p>
<p>8．为常作为查询条件的字段建立索引<br>如果某个字段经常用来做查询条件，那么该字段的查询速度会影响整个表的查询速度。因此，为这样的字段建立索引，可以提高整个表的查询速度。</p>
<p>9．限制索引的数目<br>索引的数目不是越多越好。每个索引都需要占用磁盘空间，索引越多，需要的磁盘空间就越大。修改表时，对索引的重构和更新很麻烦。越多的索引，会使更新表变得很浪费时间。</p>
<p>10．尽量使用数据量少的索引<br>如果索引的值很长，那么查询的速度会受到影响。例如，对一个CHAR(100)类型的字段进行全文检索需要的时间肯定要比对CHAR(10)类型的字段需要的时间要多。</p>
<p>11．尽量使用前缀来索引<br>如果索引字段的值很长，最好使用值的前缀来索引。例如，TEXT和BLOG类型的字段，进行全文检索会很浪费时间。如果只检索字段的前面的若干个字符，这样可以提高检索速度。</p>
<p>12．删除不再使用或者很少使用的索引<br>表中的数据被大量更新，或者数据的使用方式被改变后，原有的一些索引可能不再需要。数据库管理员应当定期找出这些索引，将它们删除，从而减少索引对更新操作的影响。</p>
<h3 id="3-索引存储原理？"><a href="#3-索引存储原理？" class="headerlink" title="3.索引存储原理？"></a>3.索引存储原理？</h3><p>虽然 InnoDB 也使用 B+Tree 作为索引结构,但具体实现方式却与 MyISAM 截然不同。<br>1.第一个重大区别是 InnoDB 的数据文件本身就是索引文件。从上文知道,MyISAM 索引文件和数据文件是分离的,索引文件仅保存数据记录的地址。</p>
<h3 id="4-mysql数据库锁有哪几种？"><a href="#4-mysql数据库锁有哪几种？" class="headerlink" title="4.mysql数据库锁有哪几种？"></a>4.mysql数据库锁有哪几种？</h3><p>表级锁和行级锁</p>
<h3 id="5-写一个数据库死锁的sql？"><a href="#5-写一个数据库死锁的sql？" class="headerlink" title="5.写一个数据库死锁的sql？"></a>5.写一个数据库死锁的sql？</h3><p>死锁是指，两个或者多个事务在同一个资源上互相占用，并请求锁定对方占用的资源，从而导致恶性循环的现象。</p>
<p>注意：当多个事务试图以不同的顺序锁定资源时，就可能会产生死锁。<br>例子：</p>
<p>事务1<br>START TRANSACTION;<br>UPDATE stock_price SET close = 45.50 WHERE stock_id = 4 and date = ‘2002-05-01’;<br>UPDATE stock_price SET close = 19.80 WHERE stock_id = 3 and date = ‘2002-05-02’;<br>COMMIT; 1<br>2<br>3<br>4</p>
<p>事务2<br>START TRANSACTION;<br>UPDATE stock_price SET high = 20.12 WHERE stock_id = 3 and date = ‘2002-05-02’;<br>UPDATE stock_price SET high = 47.20 WHERE stock_id = 4 and date = ‘2002-05-01’;<br>COMMIT; 1<br>2<br>3<br>4</p>
<p>如果凑巧，两个事务都执行了第一条UPDATE语句，更新了一行数据，同时也锁定了该行数据，接着每个事务都会尝试去执行第二条UPDATE语句，却发现该行已经被对方锁定，然后两个事务都等待对方释放锁，同时又持有对方需要的锁，则陷入死循环。（除非有外部因素介入，才可能解除死锁）</p>
<h3 id="6-如何做数据库分库分表？（mycat）"><a href="#6-如何做数据库分库分表？（mycat）" class="headerlink" title="6.如何做数据库分库分表？（mycat）"></a>6.如何做数据库分库分表？（mycat）</h3><h2 id="消息队列："><a href="#消息队列：" class="headerlink" title="消息队列："></a>消息队列：</h2><h3 id="1-RabbitMQ的exchange有哪几种？"><a href="#1-RabbitMQ的exchange有哪几种？" class="headerlink" title="1.RabbitMQ的exchange有哪几种？"></a>1.RabbitMQ的exchange有哪几种？</h3><p>四种ExchangeType分别是Direct exchange，Fanout exchange，Topic exchange和Headers exchange<br>Direct Exchange：<br>实现方式如下：</p>
<p>DirectExchange是RabbitMQ Broker的默认Exchange，它有一个特别的属性对一些简单的应用来说是非常有用的，在使用这个类型的Exchange时，可以不必指定routing key的名字，在此类型下创建的Queue有一个默认的routing key，这个routing key一般同Queue同名。</p>
<p>适用场景：<br>这种类型的Exchange，通常是将同一个message以一种循环的方式分发到不同的Queue，即不同的消费者手中，使用这种方式，值得注意的是message在消费者之间做了一个均衡，而不是说message在Queues之间做了均衡。</p>
<p>Fanout Exchange：<br> 实现方式如下：</p>
<p> 使用这种类型的Exchange，会忽略routing key的存在，直接将message广播到所有的Queue中。<br> 适用场景：<br>        第一：大型玩家在玩在线游戏的时候，可以用它来广播重大消息。这让我想到电影微微一笑很倾城中，有款游戏需要在世界上公布玩家重大消息，也许这个就是用的MQ实现的。这让我不禁佩服肖奈，人家在大学的时候就知道RabbitMQ的这种特性了。</p>
<pre><code>第二：体育新闻实时更新到手机客户端。

第三：群聊功能，广播消息给当前群聊中的所有人。</code></pre><p>Topic Exchange：<br>实现方式如下：</p>
<p>Topic Exchange是根据routing key和Exchange的类型将message发送到一个或者多个Queue中，我们经常拿他来实现各种publish/subscribe，即发布订阅，这也是我们经常使用到的ExchangeType。</p>
<p>使用场景：<br>       新闻的分类更新</p>
<pre><code>同意任务多个工作者协调完成

同一问题需要特定人员知晓</code></pre><p> Topic Exchange的使用场景很多，我们公司就在使用这种模式，将足球事件信息发布，需要使用这些事件消息的人只需要绑定对应的Exchange就可以获取最新消息。</p>
<p>Headers Exchange：<br>实现方式如下：</p>
<p> Headers Exchange不同于上面三种Exchange，它是根据Message的一些头部信息来分发过滤Message，忽略routing key的属性，如果Header信息和message消息的头信息相匹配，那么这条消息就匹配上了。</p>
<p>关于Headers Exchange我知道的并不多，但是这篇博客，我会在我持续深入理解RabbitMQ的基础上不断调整和更新，若有什么地方理解偏差，还请大家一起讨论。越来越体会到看英文资料的重要性。</p>
<h3 id="2-mq的使用场景有哪些？"><a href="#2-mq的使用场景有哪些？" class="headerlink" title="2.mq的使用场景有哪些？"></a>2.mq的使用场景有哪些？</h3><p>1.通过异步方式对系统解耦<br>比如用户注册流程 </p>
<p>2.增加系统的并发处理能力<br>以电商中的秒杀场景为例，采用同步处理：</p>
<p>用户点击秒杀<br>调用订单服务，验证库存、锁定库存<br>跳转到支付页面进行支付<br>分析一下，存在的问题：</p>
<p>验证库存、锁定库存会访问数据库<br>秒杀场景，商品数量有限，请求量非常大，每个请求来了都做以上处理，直接会把数据库压垮，导致数据库无法对外提供服务，数据库的不可用直接导致整个业务的不可用，秒杀活动打水漂。</p>
<p>大量请求会同时到达，同时去访问数据库，数据库连接有限，导致很多请求会处于等待状态，导致并发性能急剧下降<br>大量用户同时操作库存，存在争抢数据库锁的情况，容易导致死锁<br>秒杀中数量一般是有限，大量用户抢购，其实最终只有很少的用户能够抢购到</p>
<p>大家都有在银行办理业务的经验，银行处理业务的流程：领号、排队、等待叫号办理业务。</p>
<p>秒杀中我们也可以参考银行办理业务的流程：</p>
<p>用户点击描述<br>系统接受到用户请求后，生成一个唯一的编号，然后投递一条消息（秒杀下单）到mq<br>响应用户：秒杀正在处理中<br>秒杀系统从mq中拉取消息进行处理，处理完成之后告知用户，这步操作对于用户来说是异步处理的过程<br>从上面可以看出，从接受用户请求到响应用户请求，未访问数据库，只有生成编号和发送消息的操作，这部分处理速度是非常快的，不存在性能的问题，数据库也不存在压力的问题了，所有用户的请求都被作为一条消息投递到mq进行异步处理；从而解决了秒杀中同步处理遇到的各种问题。</p>
<p>其他一些使用场景<br>系统日志的处理<br>系统手机日志，异步发送到mq，日志服务队从mq中拉取消息进行各种处理，关于这个以后我们会专门讨论。<br>通过事件驱动的一些业务，也可以使用mq实现</p>
<h3 id="3-RabbitMQ的系统架构？"><a href="#3-RabbitMQ的系统架构？" class="headerlink" title="3.RabbitMQ的系统架构？"></a>3.RabbitMQ的系统架构？</h3><h3 id="4-RabbitMQ的任务分发机制有哪些？"><a href="#4-RabbitMQ的任务分发机制有哪些？" class="headerlink" title="4.RabbitMQ的任务分发机制有哪些？"></a>4.RabbitMQ的任务分发机制有哪些？</h3><h2 id="Redis："><a href="#Redis：" class="headerlink" title="Redis："></a>Redis：</h2><h3 id="1-使用redis有哪些好处"><a href="#1-使用redis有哪些好处" class="headerlink" title="1.使用redis有哪些好处?"></a>1.使用redis有哪些好处?</h3><p>1.使用redis有哪些好处?<br>1、数据存储在内存中，读写速度快；<br>2、支持的数据类型资源丰富；<br>3、支持事务，操作都是原子性操作；<br>4、可以设置数据存活的生命周期。</p>
<h3 id="2-redis相比memcached有哪些优势？"><a href="#2-redis相比memcached有哪些优势？" class="headerlink" title="2.redis相比memcached有哪些优势？"></a>2.redis相比memcached有哪些优势？</h3><p>(1) memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型<br>(2) redis的速度比memcached快很多<br>(3) redis可以持久化其数据</p>
<h3 id="3-redis常见性能问题和解决方案"><a href="#3-redis常见性能问题和解决方案" class="headerlink" title="3.redis常见性能问题和解决方案?"></a>3.redis常见性能问题和解决方案?</h3><p>(1) Master最好不要做任何持久化工作，如RDB内存快照和AOF日志文件<br>(2) 如果数据比较重要，某个Slave开启AOF备份数据，策略设置为每秒同步一次<br>(3) 为了主从复制的速度和连接的稳定性，Master和Slave最好在同一个局域网内<br>(4) 尽量避免在压力很大的主库上增加从库<br>(5) 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master &lt;- Slave1 &lt;- Slave2 &lt;- Slave3…这样的结构方便解决单点故障问题，实现Slave对Master的替换。如果Master挂了，可以立刻启用Slave1做Master，其他不变。</p>
<h3 id="4-redis集群有哪些模式？"><a href="#4-redis集群有哪些模式？" class="headerlink" title="4.redis集群有哪些模式？"></a>4.redis集群有哪些模式？</h3><p>一、主从同步/复制<br>二、哨兵模式<br>三、Cluster 集群</p>
<h3 id="5-redis中穿透，击穿与雪崩的预防及解决？"><a href="#5-redis中穿透，击穿与雪崩的预防及解决？" class="headerlink" title="5.redis中穿透，击穿与雪崩的预防及解决？"></a>5.redis中穿透，击穿与雪崩的预防及解决？</h3><p>缓存穿透:是指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。<br>解决办法：<br>1.对所有可能查询的参数以hash形式存储，在控制层先进行校验，不符合则丢弃。还有最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。<br>2.也可以采用一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。</p>
<p>缓存击穿:就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。<br>解决方式<br>也很简单，可以将热点数据设置为永远不过期；或者基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据。</p>
<p>缓存雪崩:如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，造成了缓存雪崩。<br>解决方法<br>1.在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。<br>2.可以通过缓存reload机制，预先去更新缓存，再即将发生大并发访问前手动触发加载缓存<br>不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀<br>3.做二级缓存，或者双缓存策略。A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期。</p>
<h3 id="6-redis哨兵模式集群的原理？"><a href="#6-redis哨兵模式集群的原理？" class="headerlink" title="6.redis哨兵模式集群的原理？"></a>6.redis哨兵模式集群的原理？</h3><h2 id="Spring："><a href="#Spring：" class="headerlink" title="Spring："></a>Spring：</h2><h3 id="1-IOC和AOP的实现原理？"><a href="#1-IOC和AOP的实现原理？" class="headerlink" title="1.IOC和AOP的实现原理？"></a>1.IOC和AOP的实现原理？</h3><p>IOC:控制反转，就是把对象的创建交给Spring来做<br>二、SpringIoc所使用的技术<br>1、xml配置文件<br>2、dom4j解析XML文件<br>3、工厂设计模式<br>4、反射</p>
<h3 id="2-AOP的应用场景有哪些？以及动态代理原理是什么？"><a href="#2-AOP的应用场景有哪些？以及动态代理原理是什么？" class="headerlink" title="2.AOP的应用场景有哪些？以及动态代理原理是什么？"></a>2.AOP的应用场景有哪些？以及动态代理原理是什么？</h3><p>/* spring的AOP底层是由 JDK提供的动态代理技术 和 CGLIB(动态字节码增强技术)实现。<br><strong>JDK动态代理：</strong>Jdk动态代理只针对于接口操作。<br>CGLIB：可以针对没有接口的java类和有接口的java类。*/<br>动态代理，照我的理解就是，在不修改原有类对象方法的源代码基础上，通过代理对象实现原有类对象方法的增强，也就是拓展原有类对象的功能。</p>
<h3 id="3-事务的传播属性有哪几种？"><a href="#3-事务的传播属性有哪几种？" class="headerlink" title="3.事务的传播属性有哪几种？"></a>3.事务的传播属性有哪几种？</h3><p>事务特点<br>1.原子性：一个事务中所有对数据库的操作是一个不可分割的操作序列，要么全做要么全不做<br>2.一致性：数据不会因为事务的执行而遭到破坏<br>3.隔离性：一个事物的执行，不受其他事务的干扰，即并发执行的事物之间互不干扰<br>4.持久性：一个事物一旦提交，它对数据库的改变就是永久的。</p>
<p>七个事务传播属性<br>　PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。<br>　PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。<br>　PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。<br>　PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。<br>　PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。<br>　PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。<br>　PROPAGATION_NESTED–如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。</p>
<p>五种隔离级别<br>隔离级别是指若干个并发的事务之间的隔离程度。<br>ISOLATION_DEFAULT–这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别.另外四个与JDBC的隔离级别相对应；<br>ISOLATION_READ_UNCOMMITTED–这是事务最低的隔离级别，它充许别外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。<br>ISOLATION_READ_COMMITTED–保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。这种事务隔离级别可以避免脏读出现，但是可能会出现不可重复读和幻像读。<br>ISOLATION_REPEATABLE_READ–这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。它除了保证一个事务不能读取另一个事务未提交的数据外，还保证了避免下面的情况产生(不可重复读)。<br>ISOLATION_SERIALIZABLE–这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读外，还避免了幻像读。</p>
<p>关键词<br>1)幻读：事务1读取记录时事务2增加了记录并提交，事务1再次读取时可以看到事务2新增的记录；<br>2)不可重复读取：事务1读取记录时，事务2更新了记录并提交，事务1再次读取时可以看到事务2修改后的记录；<br>3)脏读：事务1更新了记录，但没有提交，事务2读取了更新后的行，然后事务T1回滚，现在T2读取无效。</p>
<h3 id="4-bean的生命周期？"><a href="#4-bean的生命周期？" class="headerlink" title="4.bean的生命周期？"></a>4.bean的生命周期？</h3><p>Spring启动，查找并加载需要被Spring管理的bean，进行Bean的实例化<br>Bean实例化后对将Bean的引入和值注入到Bean的属性中<br>如果Bean实现了BeanNameAware接口的话，Spring将Bean的Id传递给setBeanName()方法<br>如果Bean实现了BeanFactoryAware接口的话，Spring将调用setBeanFactory()方法，将BeanFactory容器实例传入<br>如果Bean实现了ApplicationContextAware接口的话，Spring将调用Bean的setApplicationContext()方法，将bean所在应用上下文引用传入进来。<br>如果Bean实现了BeanPostProcessor接口，Spring就将调用他们的postProcessBeforeInitialization()方法。<br>如果Bean 实现了InitializingBean接口，Spring将调用他们的afterPropertiesSet()方法。类似的，如果bean使用init-method声明了初始化方法，该方法也会被调用<br>如果Bean 实现了BeanPostProcessor接口，Spring就将调用他们的postProcessAfterInitialization()方法。<br>此时，Bean已经准备就绪，可以被应用程序使用了。他们将一直驻留在应用上下文中，直到应用上下文被销毁。<br>如果bean实现了DisposableBean接口，Spring将调用它的destory()接口方法，同样，如果bean使用了destory-method 声明销毁方法，该方法也会被调用。</p>
<h3 id="5-Spring有哪些模块，分别有哪些作用和功能？"><a href="#5-Spring有哪些模块，分别有哪些作用和功能？" class="headerlink" title="5.Spring有哪些模块，分别有哪些作用和功能？"></a>5.Spring有哪些模块，分别有哪些作用和功能？</h3><ol>
<li>Spring Core： Core封装包是框架的最基础部分，提供IOC和依赖注入特性。这里的基础概念是BeanFactory，它提供对Factory模式的经典实现来消除对程序性单例模式的需要，并真正地允许你从程序逻辑中分离出依赖关系和配置。</li>
</ol>
<p>2.Spring Context: 构建于Core封装包基础上的 Context封装包，提供了一种框架式的对象访问方法，有些象JNDI注册器。Context封装包的特性得自于Beans封装包，并添加了对国际化（I18N）的支持（例如资源绑定），事件传播，资源装载的方式和Context的透明创建，比如说通过Servlet容器。</p>
<p>3．Spring DAO:  DAO (Data Access Object)提供了JDBC的抽象层，它可消除冗长的JDBC编码和解析数据库厂商特有的错误代码。 并且，JDBC封装包还提供了一种比编程性更好的声明性事务管理方法，不仅仅是实现了特定接口，而且对所有的POJOs（plain old Java objects）都适用。</p>
<p>4.Spring ORM: ORM 封装包提供了常用的“对象/关系”映射APIs的集成层。 其中包括JPA、JDO、Hibernate 和 iBatis 。利用ORM封装包，可以混合使用所有Spring提供的特性进行“对象/关系”映射，如前边提到的简单声明性事务管理。</p>
<p>5.Spring AOP: Spring的 AOP 封装包提供了符合AOP Alliance规范的面向方面的编程实现，让你可以定义，例如方法拦截器（method-interceptors）和切点（pointcuts），从逻辑上讲，从而减弱代码的功能耦合，清晰的被分离开。而且，利用source-level的元数据功能，还可以将各种行为信息合并到你的代码中。</p>
<p>6.Spring Web: Spring中的 Web 包提供了基础的针对Web开发的集成特性，例如多方文件上传，利用Servlet listeners进行IOC容器初始化和针对Web的ApplicationContext。当与WebWork或Struts一起使用Spring时，这个包使Spring可与其他框架结合。</p>
<p>7.Spring Web MVC: Spring中的MVC封装包提供了Web应用的Model-View-Controller（MVC）实现。Spring的MVC框架并不是仅仅提供一种传统的实现，它提供了一种清晰的分离模型，在领域模型代码和Web Form之间。并且，还可以借助Spring框架的其他特性。</p>
<h2 id="SpringMVC："><a href="#SpringMVC：" class="headerlink" title="SpringMVC："></a>SpringMVC：</h2><h3 id="1-SpringMVC的工作原理，举例说明流程？"><a href="#1-SpringMVC的工作原理，举例说明流程？" class="headerlink" title="1.SpringMVC的工作原理，举例说明流程？"></a>1.SpringMVC的工作原理，举例说明流程？</h3><h2 id="MyBatis："><a href="#MyBatis：" class="headerlink" title="MyBatis："></a>MyBatis：</h2><h3 id="1-Mybatis的二级缓存？"><a href="#1-Mybatis的二级缓存？" class="headerlink" title="1.Mybatis的二级缓存？"></a>1.Mybatis的二级缓存？</h3><p><a href="https://blog.csdn.net/Yang_Hui_Liang/article/details/88291752" target="_blank" rel="noopener">https://blog.csdn.net/Yang_Hui_Liang/article/details/88291752</a></p>
<h2 id="Zookeeper："><a href="#Zookeeper：" class="headerlink" title="Zookeeper："></a>Zookeeper：</h2><h3 id="1-zk的作用和原理？"><a href="#1-zk的作用和原理？" class="headerlink" title="1.zk的作用和原理？"></a>1.zk的作用和原理？</h3><h3 id="2-zk设计要满足哪些特性？分别解释一下？"><a href="#2-zk设计要满足哪些特性？分别解释一下？" class="headerlink" title="2.zk设计要满足哪些特性？分别解释一下？"></a>2.zk设计要满足哪些特性？分别解释一下？</h3><h3 id="3-zk的选举机制是什么？是否有了解Paxos算法？"><a href="#3-zk的选举机制是什么？是否有了解Paxos算法？" class="headerlink" title="3.zk的选举机制是什么？是否有了解Paxos算法？"></a>3.zk的选举机制是什么？是否有了解Paxos算法？</h3><h2 id="Nginx："><a href="#Nginx：" class="headerlink" title="Nginx："></a>Nginx：</h2><h3 id="1-什么是Nginx？Nginx的作用是什么？"><a href="#1-什么是Nginx？Nginx的作用是什么？" class="headerlink" title="1.什么是Nginx？Nginx的作用是什么？"></a>1.什么是Nginx？Nginx的作用是什么？</h3><p>Nginx是一款自由的、开源的、高性能的HTTP服务器和反向代理服务器；同时也是一个IMAP、POP3、SMTP代理服务器；Nginx可以作为一个HTTP服务器进行网站的发布处理，另外Nginx可以作为反向代理进行负载均衡的实现。</p>
<h3 id="2-Nginx-有哪些特点？"><a href="#2-Nginx-有哪些特点？" class="headerlink" title="2.Nginx 有哪些特点？"></a>2.Nginx 有哪些特点？</h3><h2 id="分布式："><a href="#分布式：" class="headerlink" title="分布式："></a>分布式：</h2><ol>
<li>单一职责、高内聚低耦合</li>
<li>服务粒度适中</li>
<li>考虑团队结构</li>
<li>以业务模型切入</li>
<li>演进式拆分</li>
<li>避免环形依赖与双向依赖</li>
</ol>
<h3 id="1-什么是分布式系统？解决什么问题？"><a href="#1-什么是分布式系统？解决什么问题？" class="headerlink" title="1.什么是分布式系统？解决什么问题？"></a>1.什么是分布式系统？解决什么问题？</h3><p>分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务。其目的是利用更多的机器，处理更多的数据。<br>（1）提升性能和并发，操作被分发到不同的分片，相互独立<br>（2）提升系统的可用性，即使部分分片不能用，其他分片不会受到影响</p>
<h3 id="2-如何提升系统吞吐量？"><a href="#2-如何提升系统吞吐量？" class="headerlink" title="2.如何提升系统吞吐量？"></a>2.如何提升系统吞吐量？</h3><h3 id="3-如何降低延迟？"><a href="#3-如何降低延迟？" class="headerlink" title="3.如何降低延迟？"></a>3.如何降低延迟？</h3><p>局域网<br>业务精简</p>
<h3 id="4-如何做故障恢复？"><a href="#4-如何做故障恢复？" class="headerlink" title="4.如何做故障恢复？"></a>4.如何做故障恢复？</h3><h3 id="5-如何做日志统一系统？"><a href="#5-如何做日志统一系统？" class="headerlink" title="5.如何做日志统一系统？"></a>5.如何做日志统一系统？</h3><p>ELK是Elasticsearch、Logstash、Kibana三大开源框架首字母大写简称。市面上也被成为Elastic Stack。其中Elasticsearch是一个基于Lucene、分布式、通过Restful方式进行交互的近实时搜索平台框架。像类似百度、谷歌这种大数据全文搜索引擎的场景都可以使用Elasticsearch作为底层支持框架，可见Elasticsearch提供的搜索能力确实强大,市面上很多时候我们简称Elasticsearch为es。Logstash是ELK的中央数据流引擎，用于从不同目标（文件/数据存储/MQ）收集的不同格式数据，经过过滤后支持输出到不同目的地（文件/MQ/redis/elasticsearch/kafka等）。Kibana可以将elasticsearch的数据通过友好的页面展示出来，提供实时分析的功能。</p>
<h3 id="6-怎么实现通讯编程？如rpc服务，webService服务等；"><a href="#6-怎么实现通讯编程？如rpc服务，webService服务等；" class="headerlink" title="6.怎么实现通讯编程？如rpc服务，webService服务等；"></a>6.怎么实现通讯编程？如rpc服务，webService服务等；</h3><h3 id="7-高并发秒杀解决方案有哪些？"><a href="#7-高并发秒杀解决方案有哪些？" class="headerlink" title="7.高并发秒杀解决方案有哪些？"></a>7.高并发秒杀解决方案有哪些？</h3><h3 id="8-分布式系统有哪些优势？"><a href="#8-分布式系统有哪些优势？" class="headerlink" title="8.分布式系统有哪些优势？"></a>8.分布式系统有哪些优势？</h3><h3 id="9-分布式系统会面临什么挑战？"><a href="#9-分布式系统会面临什么挑战？" class="headerlink" title="9.分布式系统会面临什么挑战？"></a>9.分布式系统会面临什么挑战？</h3><p>问题一：异构系统的不标准问题<br>这主要表现在：软件和应用、通讯协议、数据格式、开发和运维的过程和方法不标准。</p>
<p>不同的语言会出现不同的兼容性和不同的开发、测试、运维标准。比如：有的软件修改配置要改它的.conf 文件，而有的则是调用管理 API 接口。</p>
<p>通讯：不同的软件用不同的协议，就算是相同的网络协议里也会出现不同的数据格式。还有，不同的团队因为用不同的技术，会有不同的开发和运维方式。这些不同的东西，会让我们的整个分布式系统架构变得异常复杂。所以，分布式系统架构需要有相应的规范。…</p>
<p>数据通讯协议。通常来说，协议有协议头（最基本的协议数据）和协议体（真正的业务数据）。协议头，统一标准定义，才容易对请求进行监控、调度和管理。</p>
<p>问题二：系统架构中的服务依赖性问题。<br>如果非关键业务被关键业务所依赖，会导致非关键业务变成一个关键业务。</p>
<p>服务依赖链中，出现“木桶短板效应”——整个 SLA 由最差的那个服务所决定。</p>
<p>服务治理需要定义出服务关键程度和关键业务或服务调用的主要路径。否则无法运维或是管理整个系统。</p>
<p>数据库方面也需要做相应的隔离：避免非关键业务把数据库拖死，那么会导致全站不可用。系统间不能读取对方的数据库，只通过服务接口耦合。这也是微服务的要求：拆分服务和相应数据库。</p>
<p>问题三：故障发生的概率更大<br>故障恢复时间过长/影响面过大才可怕。</p>
<p>系统里添加各种监控指标是在“使蛮力”。信息太多等于没有信息，要定义出关键指标。</p>
<p>设计时就要考虑如何减轻故障。如果无法避免，也要使用自动化的方式恢复故障，减少故障影响面。</p>
<p>问题四：多层架构的运维复杂度更大<br>基础层：机器、网络和存储设备等。</p>
<p>平台层：中间件层，Tomcat、MySQL、Redis、Kafka 之类的软件。</p>
<p>应用层：业务软件，比如，各种功能的服务。</p>
<p>接入层：接入用户请求的网关、负载均衡或是 CDN、DNS 这样的东西。</p>
<p>没有统一的视图和管理，导致运维被割裂开来，造成更大的复杂度。</p>
<p>技术团队分为产品开发、中间件开发、业务运维、系统运维等子团队。整个系统会像 “多米诺骨牌”一样，一个环节出现问题，就会倒下去一大片。因为没有一个统一的运维视图，不知道一个服务调用是如何经过每一个服务和资源，花大量的时间在沟通和定位问题上。</p>
<p>分工不是问题，问题是分工后的协作是否统一和规范。</p>
<h3 id="10-如何设计分布式系统？"><a href="#10-如何设计分布式系统？" class="headerlink" title="10.如何设计分布式系统？"></a>10.如何设计分布式系统？</h3><p>功能单一性<br>服务独立性</p>
<h3 id="11-如何做分布式事务？"><a href="#11-如何做分布式事务？" class="headerlink" title="11.如何做分布式事务？"></a>11.如何做分布式事务？</h3><p>一般来说，分布式事务的实现主要有以下 5 种方案：<br>XA 方案<br>TCC 方案<br>本地消息表<br>可靠消息最终一致性方案<br>最大努力通知方案</p>
<p>两阶段提交方案/XA方案<br>所谓的 XA 方案，即：两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复 ok，那么就正式提交事务，在各个数据库上执行操作；如果任何其中一个数据库回答不 ok，那么就回滚事务。</p>
<p>这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于 Spring + JTA 就可以搞定，自己随便搜个 demo 看看就知道了。</p>
<p>这个方案，我们很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。我可以给大家介绍一下， 现在微服务，一个大的系统分成几十个甚至几百个服务。一般来说，我们的规定和规范，是要求每个服务只能操作自己对应的一个数据库。</p>
<p>如果你要操作别的服务对应的库，不允许直连别的服务的库，违反微服务架构的规范，你随便交叉胡乱访问，几百个服务的话，全体乱套，这样的一套服务是没法管理的，没法治理的，可能会出现数据被别人改错，自己的库被别人写挂等情况。</p>
<p>如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许交叉访问别人的数据库。</p>
<p>​</p>
<p>TCC 方案<br>TCC 的全称是：Try、Confirm、Cancel。</p>
<p>Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留。<br>Confirm 阶段：这个阶段说的是在各个服务中执行实际的操作。<br>Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）<br>这种方案说实话几乎很少人使用，我们用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。</p>
<p>比如说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用 TCC，严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性，保证在资金上不会出现问题。</p>
<p>而且最好是你的各个业务执行的时间都比较短。</p>
<p>但是说实话，一般尽量别这么搞，自己手写回滚逻辑，或者是补偿逻辑，实在太恶心了，那个业务代码很难维护。</p>
<p>​</p>
<p>本地消息表<br>本地消息表其实是国外的 ebay 搞出来的这么一套思想。</p>
<p>这个大概意思是这样的：</p>
<p>A 系统在自己本地一个事务里操作同时，插入一条数据到消息表；<br>接着 A 系统将这个消息发送到 MQ 中去；<br>B 系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息；<br>B 系统执行成功之后，就会更新自己本地消息表的状态以及 A 系统消息表的状态；<br>如果 B 系统处理失败了，那么就不会更新消息表状态，那么此时 A 系统会定时扫描自己的消息表，如果有未处理的消息，会再次发送到 MQ 中去，让 B 再次处理；<br>这个方案保证了最终一致性，哪怕 B 事务失败了，但是 A 会不断重发消息，直到 B 那边成功为止。<br>这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的，会导致如果是高并发场景咋办呢？咋扩展呢？所以一般确实很少用。</p>
<p>​</p>
<p>可靠消息最终一致性方案<br>这个的意思，就是干脆不要用本地的消息表了，直接基于 MQ 来实现事务。比如阿里的 RocketMQ 就支持消息事务。</p>
<p>大概的意思就是：</p>
<p>A 系统先发送一个 prepared 消息到 mq，如果这个 prepared 消息发送失败那么就直接取消操作别执行了；<br>如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉 mq 发送确认消息，如果失败就告诉 mq 回滚消息；<br>如果发送了确认消息，那么此时 B 系统会接收到确认消息，然后执行本地的事务；<br>mq 会自动定时轮询所有 prepared 消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认的消息，是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。<br>这个方案里，要是系统 B 的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如 B 系统本地回滚后，想办法通知系统 A 也回滚；或者是发送报警由人工来手工回滚和补偿。<br>这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你举用 RocketMQ 支持的，要不你就自己基于类似 ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的。<br>​</p>
<p>最大努力通知方案<br>这个方案的大致意思就是：</p>
<p>系统 A 本地事务执行完之后，发送个消息到 MQ；<br>这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统 B 的接口；<br>要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃。<br>你们公司是如何处理分布式事务的？<br>如果你真的被问到，可以这么说，我们某某特别严格的场景，用的是 TCC 来保证强一致性；然后其他的一些场景基于阿里的 RocketMQ 来实现分布式事务。</p>
<p>你找一个严格资金要求绝对不能错的场景，你可以说你是用的 TCC 方案；如果是一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案。</p>
<p>友情提示一下，RocketMQ 3.2.6 之前的版本，是可以按照上面的思路来的，但是之后接口做了一些改变，我这里不再赘述了。</p>
<p>当然如果你愿意，你可以参考可靠消息最终一致性方案来自己实现一套分布式事务，比如基于 RocketMQ 来玩儿。</p>
<h2 id="其他问题："><a href="#其他问题：" class="headerlink" title="其他问题："></a>其他问题：</h2><h3 id="1-如何将一个请求由原来的10s减少到3s？可以从哪些地方优化？"><a href="#1-如何将一个请求由原来的10s减少到3s？可以从哪些地方优化？" class="headerlink" title="1.如何将一个请求由原来的10s减少到3s？可以从哪些地方优化？"></a>1.如何将一个请求由原来的10s减少到3s？可以从哪些地方优化？</h3><h3 id="2-如何支持大量流量的访问？可以在哪些地方进行优化？"><a href="#2-如何支持大量流量的访问？可以在哪些地方进行优化？" class="headerlink" title="2.如何支持大量流量的访问？可以在哪些地方进行优化？"></a>2.如何支持大量流量的访问？可以在哪些地方进行优化？</h3><h3 id="3-双11流量怎么控制？"><a href="#3-双11流量怎么控制？" class="headerlink" title="3.双11流量怎么控制？"></a>3.双11流量怎么控制？</h3><p><a href="https://www.cnblogs.com/zhuifeng523/p/11837663.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhuifeng523/p/11837663.html</a></p>
<h3 id="4-1亿无序的数据文件，如何找出最小的10个数并去重？（topk算法）"><a href="#4-1亿无序的数据文件，如何找出最小的10个数并去重？（topk算法）" class="headerlink" title="4.1亿无序的数据文件，如何找出最小的10个数并去重？（topk算法）"></a>4.1亿无序的数据文件，如何找出最小的10个数并去重？（topk算法）</h3><h3 id="5-分布式环境下，如何对一个web请求的做监控？"><a href="#5-分布式环境下，如何对一个web请求的做监控？" class="headerlink" title="5.分布式环境下，如何对一个web请求的做监控？"></a>5.分布式环境下，如何对一个web请求的做监控？</h3>]]></content>
  </entry>
  <entry>
    <title>Java程序员Chrome浏览器必装插件</title>
    <url>/post/b0de6b16/</url>
    <content><![CDATA[<p>整理了一下自己一直在用的Chrome浏览器插件,之后有觉得好用的也会更新进来</p>
<h2 id="SQLDelimiter"><a href="#SQLDelimiter" class="headerlink" title="SQLDelimiter"></a><a href="https://chrome.google.com/webstore/detail/sqldelimiter/ngobnmgdnggmcafeblbealjjnpakoghd" target="_blank" rel="noopener">SQLDelimiter</a></h2><p>参数拆分插件,从Excel直接复制或者MySQL查询结果复制,直接转化成带逗号分隔<br><img data-src="/images/loading.png" data-original="/images/pasted-3.png" alt="upload successful"></p>
<h2 id="JSON-handle"><a href="#JSON-handle" class="headerlink" title="JSON-handle"></a><a href="https://chrome.google.com/webstore/detail/json-handle/iahnhfdhidomcpggpaimmmahffihkfnj" target="_blank" rel="noopener">JSON-handle</a></h2><p>Json格式化或者转成更好看的形式,方便检查数据<br><img data-src="/images/loading.png" data-original="/images/pasted-4.png" alt="upload successful"></p>
<h2 id="JSON-Formatter"><a href="#JSON-Formatter" class="headerlink" title="JSON Formatter"></a><a href="https://chrome.google.com/webstore/detail/json-formatter/bcjindcccaagfpapjjmafapmmgkkhgoa" target="_blank" rel="noopener">JSON Formatter</a></h2><p>直接格式化http请求返回的json内容</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Chrome</tag>
        <tag>插件</tag>
        <tag>开发</tag>
      </tags>
  </entry>
  <entry>
    <title>程序员必须知道的linux命令</title>
    <url>/post/b77c3e4/</url>
    <content><![CDATA[<h3 id="查找文件"><a href="#查找文件" class="headerlink" title="查找文件"></a>查找文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find / -name filename.txt 根据名称查找/目录下的filename.txt文件。</span><br><span class="line">find . -name &quot;*.xml&quot; 递归查找所有的xml文件</span><br><span class="line">find .  -name &quot;*.xml&quot; |xargs grep  &quot;hello world&quot; 递归查找所有文件内容中包含hello world的xml文件</span><br><span class="line">grep -H  &apos;spring&apos; *.xml 查找所以有的包含spring的xml文件</span><br><span class="line">find ./ -size 0 | xargs rm -f &amp; 删除文件大小为零的文件</span><br><span class="line">ls -l | grep &apos;jar&apos; 查找当前目录中的所有jar文件</span><br><span class="line">grep &apos;test&apos; d* 显示所有以d开头的文件中包含test的行。</span><br><span class="line">grep &apos;test&apos; aa bb cc 显示在aa，bb，cc文件中匹配test的行。</span><br><span class="line">grep &apos;[a-z]\&#123;5\&#125;&apos; aa 显示所有包含每个字符串至少有5个连续小写字符的字符串的行。</span><br></pre></td></tr></table></figure>
<h3 id="查看一个程序是否运行"><a href="#查看一个程序是否运行" class="headerlink" title="查看一个程序是否运行"></a>查看一个程序是否运行</h3><p><code>ps –ef|grep tomcat 查看所有有关tomcat的进程</code></p>
<h3 id="终止线程"><a href="#终止线程" class="headerlink" title="终止线程"></a>终止线程</h3><p><code>kill -9 19979 终止线程号位19979的线程</code></p>
<h3 id="查看文件，包含隐藏文件"><a href="#查看文件，包含隐藏文件" class="headerlink" title="查看文件，包含隐藏文件"></a>查看文件，包含隐藏文件</h3><p><code>ls -al</code></p>
<h3 id="当前工作目录"><a href="#当前工作目录" class="headerlink" title="当前工作目录"></a>当前工作目录</h3><p><code>pwd</code></p>
<h3 id="复制文件"><a href="#复制文件" class="headerlink" title="复制文件"></a>复制文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp  source dest 复制文件</span><br><span class="line">cp -r  sourceFolder targetFolder 递归复制整个文件夹</span><br><span class="line">scp sourecFile romoteUserName@remoteIp:remoteAddr 远程拷贝</span><br></pre></td></tr></table></figure>
<h3 id="创建目录"><a href="#创建目录" class="headerlink" title="创建目录"></a>创建目录</h3><p><code>mkdir newfolder</code></p>
<h3 id="删除目录"><a href="#删除目录" class="headerlink" title="删除目录"></a>删除目录</h3><p><code>rmdir deleteEmptyFolder 删除空目录 rm -rf deleteFile 递归删除目录中所有内容</code></p>
<h3 id="移动文件"><a href="#移动文件" class="headerlink" title="移动文件"></a>移动文件</h3><p><code>mv /temp/movefile /targetFolder</code></p>
<h3 id="重命令"><a href="#重命令" class="headerlink" title="重命令"></a>重命令</h3><p><code>mv oldNameFile newNameFile</code></p>
<h3 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h3><p><code>su -username</code></p>
<h3 id="修改文件权限"><a href="#修改文件权限" class="headerlink" title="修改文件权限"></a>修改文件权限</h3><p><code>chmod 777 file.java //file.java的权限-rwxrwxrwx，r表示读、w表示写、x表示可执行</code></p>
<h3 id="压缩文件"><a href="#压缩文件" class="headerlink" title="压缩文件"></a>压缩文件</h3><p><code>tar -czf test.tar.gz /test1 /test2</code></p>
<h3 id="列出压缩文件列表"><a href="#列出压缩文件列表" class="headerlink" title="列出压缩文件列表"></a>列出压缩文件列表</h3><p><code>tar -tzf test.tar.gz</code></p>
<h3 id="解压文件"><a href="#解压文件" class="headerlink" title="解压文件"></a>解压文件</h3><p><code>tar -xvzf test.tar.gz</code></p>
<h3 id="查看文件头10行"><a href="#查看文件头10行" class="headerlink" title="查看文件头10行"></a>查看文件头10行</h3><p><code>head -n 10 example.txt</code></p>
<h3 id="查看文件尾10行"><a href="#查看文件尾10行" class="headerlink" title="查看文件尾10行"></a>查看文件尾10行</h3><p><code>tail -n 10 example.txt</code></p>
<h3 id="查看日志类型文件"><a href="#查看日志类型文件" class="headerlink" title="查看日志类型文件"></a>查看日志类型文件</h3><p><code>tail -f exmaple.log //这个命令会自动显示新增内容，屏幕只显示10行内容的（可设置）。</code></p>
<h3 id="使用超级管理员身份执行命令"><a href="#使用超级管理员身份执行命令" class="headerlink" title="使用超级管理员身份执行命令"></a>使用超级管理员身份执行命令</h3><p><code>sudo rm a.txt 使用管理员身份删除文件</code></p>
<h3 id="查看端口占用情况"><a href="#查看端口占用情况" class="headerlink" title="查看端口占用情况"></a>查看端口占用情况</h3><p><code>netstat -tln | grep 8080 查看端口8080的使用情况</code></p>
<h3 id="查看端口属于哪个程序"><a href="#查看端口属于哪个程序" class="headerlink" title="查看端口属于哪个程序"></a>查看端口属于哪个程序</h3><p><code>lsof -i :8080</code></p>
<h3 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps aux|grep java 查看java进程</span><br><span class="line">ps aux 查看所有进程</span><br></pre></td></tr></table></figure>
<h3 id="以树状图列出目录的内容"><a href="#以树状图列出目录的内容" class="headerlink" title="以树状图列出目录的内容"></a>以树状图列出目录的内容</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tree a</span><br><span class="line">ps:Mac下使用tree命令</span><br></pre></td></tr></table></figure>
<h3 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http://file.tgz mac下安装wget命令</span><br><span class="line">curl http://file.tgz</span><br></pre></td></tr></table></figure>
<h3 id="网络检测"><a href="#网络检测" class="headerlink" title="网络检测"></a>网络检测</h3><p><code>ping www.taobao.com</code></p>
<h3 id="远程登录"><a href="#远程登录" class="headerlink" title="远程登录"></a>远程登录</h3><p><code>ssh userName@ip</code></p>
<h3 id="打印信息"><a href="#打印信息" class="headerlink" title="打印信息"></a>打印信息</h3><p>echo $JAVA_HOME 打印java home环境变量的值</p>
<h3 id="java-常用命令"><a href="#java-常用命令" class="headerlink" title="java 常用命令"></a>java 常用命令</h3><p><code>java javac jps ,jstat ,jmap, jstack</code></p>
<h3 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h3><p><code>svn git maven</code></p>
<h3 id="linux命令学习网站"><a href="#linux命令学习网站" class="headerlink" title="linux命令学习网站:"></a>linux命令学习网站:</h3><p><a href="http://explainshell.com/" target="_blank" rel="noopener">http://explainshell.com/</a></p>
<h3 id="修改机器名"><a href="#修改机器名" class="headerlink" title="修改机器名"></a>修改机器名</h3><pre><code>更改/etc/sysconfig下的network文件，在提示符下输入vi /etc/sysconfig/network，然后将HOSTNAME后面的值改为想要设置的主机名，在提示符下输入reboot命令，重新启动服务器。</code></pre><h3 id="解压nginx日志"><a href="#解压nginx日志" class="headerlink" title="解压nginx日志"></a>解压nginx日志</h3><pre><code>现在nginx日志用gzip压缩，文件以gz结尾。

解压命令： gzip -d  压缩文件名 
不解压，直接查看压缩文件内容，命令： gunzip -c  压缩文件名</code></pre><h3 id="测试磁盘的读写"><a href="#测试磁盘的读写" class="headerlink" title="测试磁盘的读写"></a>测试磁盘的读写</h3><pre><code> time有计时作用，dd用于复制，从if读出，写到of。if=/dev/zero不产生IO，因此可以用来测试纯写速度。同理of=/dev/null不产生IO，可以用来测试纯读速度。bs是每次读或写的大小，即一个块的大小，count是读写块的数量。

纯写速度： time dd if=/dev/zero of=/var/test bs=8k count=10000

 纯读速度：time dd if=/var/test of=/dev/null bs=8k</code></pre><h3 id="linux端口连接范围-gt-gt-gt-gt-gt-用于向外连接的端口范围"><a href="#linux端口连接范围-gt-gt-gt-gt-gt-用于向外连接的端口范围" class="headerlink" title="linux端口连接范围&gt;&gt;&gt;&gt;&gt;用于向外连接的端口范围"></a>linux端口连接范围&gt;&gt;&gt;&gt;&gt;用于向外连接的端口范围</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat /proc/sys/net/ipv4/ip_local_port_range</span><br><span class="line">net.ipv4.ip_local_port_range=1024 65000</span><br></pre></td></tr></table></figure>
<h3 id="修改打开文件最大数，vi-etc-security-limits-conf-加入以下两行"><a href="#修改打开文件最大数，vi-etc-security-limits-conf-加入以下两行" class="headerlink" title="修改打开文件最大数，vi /etc/security/limits.conf,加入以下两行"></a>修改打开文件最大数，vi /etc/security/limits.conf,加入以下两行</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* soft nofile 402400</span><br><span class="line">* hard nofile 480000</span><br></pre></td></tr></table></figure>
<p>然后重新登录即可使用命令ulimit -a 查看到修改。在这之前启动的进程仍旧使用以前的配置，之后启动的进程使用最新配置<br>用ulimit -n 2048 修改只对当前的shell有效，退出后失效</p>
<h3 id="tcp连接复用和快速回收"><a href="#tcp连接复用和快速回收" class="headerlink" title="tcp连接复用和快速回收"></a>tcp连接复用和快速回收</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">net.ipv4.tcp_tw_reuse = 1</span><br><span class="line">net.ipv4.tcp_tw_recycle = 1</span><br></pre></td></tr></table></figure>
<p><code>net.ipv4.tcp_keepalive_time = 1200</code>表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。</p>
<p><code>net.ipv4.ip_local_port_range = 1024</code> 65535表示用于向外连接的端口范围。缺省情况下过窄：32768到61000，改为1024到65535。</p>
<p><code>net.ipv4.tcp_max_syn_backlog = 16384</code><br>表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。 </p>
<p><code>net.ipv4.tcp_max_tw_buckets = 180000</code> </p>
<p>表示系统同时保持TIME_WAIT套接字的最大数 量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000,可适当增大该值，但不建议减小。对于Apache、 Nginx等服务器，以上几行参数的设置可以很好地减少TIME_WAIT套接字数量，但是对于Squid，效果却不大。此项参数可以控制 TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">net.ipv4.tcp_keepalive_time = 180</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 15</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 2</span><br><span class="line">net.ipv4.tcp_fin_timeout = 5</span><br><span class="line"></span><br><span class="line">nf_conntrack: table full, dropping packet.</span><br></pre></td></tr></table></figure>


<h3 id="查看NS提供商"><a href="#查看NS提供商" class="headerlink" title="查看NS提供商"></a>查看NS提供商</h3><p><code>dig baidu.com NS trace @8.8.8.8</code></p>
<h3 id="指定NS解析域名"><a href="#指定NS解析域名" class="headerlink" title="指定NS解析域名"></a>指定NS解析域名</h3><p> <code>nslookup www.baidu.com</code></p>
<h3 id="sz-rz安装"><a href="#sz-rz安装" class="headerlink" title="sz/rz安装"></a>sz/rz安装</h3><p>（1）编译安装</p>
<p>root 账号登陆后，依次执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /tmp</span><br><span class="line"></span><br><span class="line">wget http://www.ohse.de/uwe/releases/lrzsz-0.12.20.tar.gz</span><br><span class="line"></span><br><span class="line">tar zxvf lrzsz-0.12.20.tar.gz &amp;&amp; cd lrzsz-0.12.20</span><br><span class="line"></span><br><span class="line">./configure &amp;&amp; make &amp;&amp; make install</span><br></pre></td></tr></table></figure>
<p>上面安装过程默认把lsz和lrz安装到了/usr/local/bin/目录下，现在我们并不能直接使用，下面创建软链接，并命名为rz/sz：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /usr/bin</span><br><span class="line">ln -s /usr/local/bin/lrz rz</span><br><span class="line">ln -s /usr/local/bin/lsz sz</span><br></pre></td></tr></table></figure>


<p>（2）yum安装</p>
<p>root 账号登陆后执行以下命令：</p>
<p><code>yum install -y lrzsz</code></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Meno</tag>
      </tags>
  </entry>
  <entry>
    <title>Mysql数据库配置优化</title>
    <url>/post/692ce8b8/</url>
    <content><![CDATA[<h1 id="tmp-table-size-临时表-用于排序"><a href="#tmp-table-size-临时表-用于排序" class="headerlink" title="tmp_table_size 临时表 (用于排序)"></a>tmp_table_size 临时表 (用于排序)</h1><p>show global status like ‘created_tmp%’; </p>
<p>| Variable_name | Value | </p>
<p>| Created_tmp_disk_tables | 21197 | #在磁盘上创建临时表的次数 </p>
<p>| Created_tmp_files | 58 | #在磁盘上创建临时文件的次数</p>
<p>| Created_tmp_tables | 1771587 | #使用临时表的总次数</p>
<p>TmpTable的状况主要是用于监控MySQL使用临时表的量是否过多，</p>
<p>是否有临时表过大而不得不从内存中换出到磁盘文件上。 </p>
<p>a.如果：</p>
<p>Created_tmp_disk_tables/Created_tmp_tables&gt;10%，则需调大tmp_table_size</p>
<p>比较理想的配置是：</p>
<p>Created_tmp_disk_tables/Created_tmp_tables&lt;=25%</p>
<p>b.如果：</p>
<p>Created_tmp_tables非常大 ，则可能是系统中排序操作过多，或者是表连接方式不是很优化。</p>
<p>相关参数：</p>
<p>tmp_table_size 内存中，临时表区域总大小</p>
<p>max_heap_table_size 内存中，单个临时表的最大值，超过的部分会放到硬盘上。</p>
<h1 id="连接数"><a href="#连接数" class="headerlink" title="连接数:"></a>连接数:</h1><p>max_connections MySQL最大连接数</p>
<p>back_log 当连接数满了后，设置一个值，允许多少个连接进入等待堆栈</p>
<p>max_connect_errors 账号连接到服务器允许的错误次数</p>
<p>connect_timeout 一个连接报文的最大时间(单位：s)</p>
<p>skip-name-resolve 加入my.cnf即可，MySQL在收到连接请求的时候，会根据请求包 </p>
<p>中获得的ip来反向追查请求者的主机名。然后再根据返回</p>
<p>的主机名又一次去获取ip。如果两次获得的ip相同，那么连接就成功建立了。</p>
<p>加了次参数，即可省去这个步骤</p>
<p>NOTES:</p>
<p>查询当前连接数:show global status like ‘connections’;</p>
<h1 id="table-cache相关优化-："><a href="#table-cache相关优化-：" class="headerlink" title="table cache相关优化 ："></a>table cache相关优化 ：</h1><p>参数table_open_cache，将表的文件描述符打开，cache在内存中</p>
<p>global status：</p>
<p>open_tables 当前系统中打开的文件描述符的数量</p>
<p>opened_tables 系统打开过的文件描述符的数量</p>
<h1 id="慢查询日志："><a href="#慢查询日志：" class="headerlink" title="慢查询日志："></a>慢查询日志：</h1><p>slow_launch_time=2 查询大于某个时间的值(单位：s)</p>
<p>slow_query_log=on/off 开启关闭慢查询日志</p>
<p>slow_query_log_file=/opt/data/xxx.log 慢查询日志位置</p>
<h1 id="查询缓存-Query-Cache"><a href="#查询缓存-Query-Cache" class="headerlink" title="查询缓存(Query Cache)"></a>查询缓存(Query Cache)</h1><p>将客户端的SQL语句(仅限select语句)通过hash计算，放在hash链表中，同时将该SQL的结果集</p>
<p>放在内存中cache。该hash链表中，存放了结果集的内存地址以及所涉及到的所有Table等信息。</p>
<p>如果与该结果集相关的任何一个表的相关信息发生变化后(包扩：数据、索引、表结构等)，</p>
<p>就会导致结果集失效，释放与该结果集相关的所有资源，以便后面其他SQL能够使用。</p>
<p>当客户端有select SQL进入，先计算hash值，如果有相同的，就会直接将结果集返回。</p>
<p>Query Cache的负面影响：</p>
<p>a.使用了Query Cache后，每条select SQL都要进行hash计算，然后查找结果集。对于大量SQL</p>
<p>访问，会消耗过多额外的CPU。</p>
<p>b.如果表变更比较频繁，则会造成结果集失效率非常高。</p>
<p>c.结果集中保存的是整个结果，可能存在一条记录被多次cache的情况，这样会造成内存资源的</p>
<p>过度消耗。</p>
<p>Query Cache的正确使用：</p>
<p>a.根据表的变更情况来选择是否使用Query Cache，可使用SQL Hint：SQL_NO_CACHE和SQL_CACHE</p>
<p>b.对于 变更比较少 或 数据基本处于静态 的表，使用SQL_CACHE</p>
<p>c.对于结果集比较大的，使用Query Cache可能造成内存不足，或挤占内存。</p>
<p>可使用1.SQL_NO_CACHE 2.query_cache_limit控制Query Cache的最大结果集(系统默认1M)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like &apos;%query_cache%&apos;;</span><br><span class="line"></span><br><span class="line">+------------------------------+---------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+------------------------------+---------+</span><br><span class="line"></span><br><span class="line">| have_query_cache | YES | #是否支持Query Cache</span><br><span class="line"></span><br><span class="line">| query_cache_limit | 1048576 | #单个结果集的最大值，默认1M</span><br><span class="line"></span><br><span class="line">| query_cache_min_res_unit | 4096 | #每个结果集存放的最小内存，默认4K</span><br><span class="line"></span><br><span class="line">| query_cache_size | 0 | #Query Cache总内存大小，必须是1024的整数倍</span><br><span class="line"></span><br><span class="line">| query_cache_type | ON | #ON,OFF,DEMAND(包含SQL_CACHE的查询中才开启)</span><br><span class="line"></span><br><span class="line">| query_cache_wlock_invalidate | OFF | </span><br><span class="line"></span><br><span class="line">+------------------------------+---------+</span><br><span class="line"></span><br><span class="line">#query_cache_wlock_invalidate：</span><br></pre></td></tr></table></figure>

<p>针对于MyISAM存储引擎，设置当有WRITELOCK在某个Table上面的时候，</p>
<p>读请求是要等待WRITE LOCK释放资源之后再查询还是允许直接从QueryCache中读取结果，</p>
<p>默认为FALSE（可以直接从QueryCache中取得结果）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like &apos;qcache%&apos;;</span><br><span class="line"></span><br><span class="line">+-------------------------+-------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+-------------------------+-------+</span><br><span class="line"></span><br><span class="line">| Qcache_free_blocks | 0 | </span><br><span class="line"></span><br><span class="line">| Qcache_free_memory | 0 |</span><br><span class="line"></span><br><span class="line">| Qcache_hits | 0 |</span><br><span class="line"></span><br><span class="line">| Qcache_inserts | 0 |</span><br><span class="line"></span><br><span class="line">| Qcache_lowmem_prunes | 0 |</span><br><span class="line"></span><br><span class="line">| Qcache_not_cached | 0 |</span><br><span class="line"></span><br><span class="line">| Qcache_queries_in_cache | 0 |</span><br><span class="line"></span><br><span class="line">| Qcache_total_blocks | 0 |</span><br><span class="line"></span><br><span class="line">+-------------------------+-------+</span><br><span class="line"></span><br><span class="line"># Qcache_free_blocks</span><br></pre></td></tr></table></figure>

<p>QueryCache中目前还有多少剩余的blocks</p>
<p>a.如果Qcache_free_blocks值较大，说明Query Cache中内存碎片比较多</p>
<p>b.如果Qcache_free_blocks约等于Qcache_total_blocks/2，说明内存碎片非常严重</p>
<p>移除碎片：</p>
<p>flush query cache;</p>
<p>这个命令会把所有的存储块向上移动，并把自由块移到底部。</p>
<p>查询缓存碎片率：</p>
<p>查询缓存碎片率 = Qcache_free_blocks / Qcache_total_blocks * 100%</p>
<p>c.如果：</p>
<p>查询缓存碎片率超过20%， 可以用flush query cache整理碎片，或者减小</p>
<p>query_cache_min_res_unit(如果该系统的查询都是小数据量的话)</p>
<h2 id="Qcache-free-memory"><a href="#Qcache-free-memory" class="headerlink" title="Qcache_free_memory"></a>Qcache_free_memory</h2><p>QueryCache中目前剩余的内存大小</p>
<p>查询缓存利用率：</p>
<p>查询缓存利用率 = (query_cache_size – Qcache_free_memory) / query_cache_size * 100%</p>
<p>a.如果：</p>
<p>查询缓存利用率在25%以下，说明query_cache_size设置过大，可适当减小。</p>
<p>b.如果：</p>
<p>查询缓存利用率&gt;80%，且Qcache_lowmem_prunes&gt;50，说明query_cache_size可能有点小，或者</p>
<p>有太多的碎片</p>
<h2 id="Qcache-hits"><a href="#Qcache-hits" class="headerlink" title="Qcache_hits"></a>Qcache_hits</h2><p>Query Cache的命中次数，可以看到QueryCache的基本效果；</p>
<h2 id="Qcache-inserts"><a href="#Qcache-inserts" class="headerlink" title="Qcache_inserts"></a>Qcache_inserts</h2><p>Query Cache未命中然后插入的次数</p>
<p>Query Cache的命中率：</p>
<p>=Qcache_hits/(Qcache_hits+Qcache_inserts)</p>
<h2 id="Qcache-lowmem-prunes"><a href="#Qcache-lowmem-prunes" class="headerlink" title="Qcache_lowmem_prunes"></a>Qcache_lowmem_prunes</h2><p>因为内存不足而被清除出Query Cache的SQL数量。</p>
<p>如果：</p>
<p>Qcache_lowmem_prunes的值正在增加，并且有大量的Qcache_free_blocks，</p>
<p>这意味着碎片导致查询正在被从缓存中永久删除。</p>
<h2 id="Qcache-not-cached"><a href="#Qcache-not-cached" class="headerlink" title="Qcache_not_cached"></a>Qcache_not_cached</h2><p>因为query_cache_type的设置或者不能被cache的select SQL数量</p>
<h2 id="Qcache-queries-in-cache"><a href="#Qcache-queries-in-cache" class="headerlink" title="Qcache_queries_in_cache"></a>Qcache_queries_in_cache</h2><p>Query Cache中cache的select SQL数量</p>
<h2 id="Qcache-total-blocks"><a href="#Qcache-total-blocks" class="headerlink" title="Qcache_total_blocks"></a>Qcache_total_blocks</h2><p>当前Query Cache中block的总数量</p>
<p>Query Cache限制：</p>
<p>a) 5.1.17之前的版本不能Cache帮定变量的Query，但是从5.1.17版本开始，QueryCache已经开</p>
<p>始支持帮定变量的Query了； </p>
<p>b) 所有子查询中的外部查询SQL不能被Cache； </p>
<p>c) 在Procedure，Function以及Trigger中的Query不能被Cache； </p>
<p>d) 包含其他很多每次执行可能得到不一样结果的函数的Query不能被Cache。</p>
<h1 id="进程的使用情况"><a href="#进程的使用情况" class="headerlink" title="进程的使用情况"></a>进程的使用情况</h1><p>在MySQL中，为了尽可能提高客户端请求创建连接这个过程的性能，实现了一个ThreadCache池，</p>
<p>将空闲的连接线程存放在其中，而不是完成请求后就销毁。这样，当有新的连接请求的时候，</p>
<p>MySQL首先会检查ThreadCache池中是否存在空闲连接线程，如果存在则取出来直接使用，</p>
<p>如果没有空闲连接线程，才创建新的连接线程。</p>
<p>参数：thread_cache_size</p>
<p>thread cache 池中存放的最大连接数</p>
<p>调整参考：</p>
<p>在短连接的数据库应用中，数据库连接的创建和销毁是非常频繁的，</p>
<p>如果每次都需要让MySQL新建和销毁相应的连接线程，那么这个资源消耗实际上是非常大的，因此</p>
<p>thread_cache_size的值应该设置的相对大一些，不应该小于应用系统对数据库的实际并发请求数。</p>
<p>参数：thread_stack - 每个连接线程被创建的时候，MySQL给他分配的内存大小，</p>
<p>类似PGA中存放数据的内存部分(不包括排序的空间)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">show status like &apos;connections&apos;;</span><br><span class="line"></span><br><span class="line">+---------------+-------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+---------------+-------+</span><br><span class="line"></span><br><span class="line">| Connections | 80 | #接受到的来自客户端的总连接数，包括以前和现在的连接。</span><br><span class="line"></span><br><span class="line">+---------------+-------+</span><br><span class="line"></span><br><span class="line">show status like &apos;thread%&apos;;</span><br><span class="line"></span><br><span class="line">+-------------------+-------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+-------------------+-------+</span><br><span class="line"></span><br><span class="line">| Threads_cached | 0 | #当前系统中，缓存的连接数</span><br><span class="line"></span><br><span class="line">| Threads_connected | 1 | #当前系统中正连接的线程数</span><br><span class="line"></span><br><span class="line">| Threads_created | 77 | #创建过的总线程数</span><br><span class="line"></span><br><span class="line">| Threads_running | 1 | </span><br><span class="line"></span><br><span class="line">+-------------------+-------+</span><br></pre></td></tr></table></figure>
<p>a.如果：</p>
<p>Threads_created 值过大，说明MySQL一直在创建线程，这是比较消耗资源的，应该适当增大</p>
<p>thread_cache_size的值</p>
<p>b.如果：</p>
<p>Threads_cached的值比参数thread_cache_size小太多，则可以适当减小thread_cache_size的值</p>
<p>ThreadCache命中率：</p>
<p>Threads_Cache_Hit=(Connections-Threads_created)/Connections*100%</p>
<p>一般来说，当系统稳定运行一段时间之后，我们的ThreadCache命中率应该保持在90%</p>
<p>左右甚至更高的比率才算正常。</p>
<h1 id="key-buffer-size"><a href="#key-buffer-size" class="headerlink" title="key_buffer_size"></a>key_buffer_size</h1><p>索引缓存大小，是对MyISAM表性能影响最大的一个参数</p>
<p>32bit平台上，此值不要超过2GB，64bit平台不用做此限制，但也不要超过4GB</p>
<p>根据3点计算：</p>
<p>a.系统索引总大小 </p>
<p>b.系统物理内存 </p>
<p>c.系统当前keycache命中率</p>
<p>粗略计算公式：</p>
<p>Key_Size =key_number*(key_length+4)/0.67 </p>
<p>Max_key_buffer_size&lt;Max_RAM-QCache_Usage-Threads_Usage-System_Usage </p>
<p>Threads_Usage = max_connections * (sort_buffer_size + join_buffer_size + </p>
<p>read_buffer_size+read_rnd_buffer_size+thread_stack)</p>
<p>key_cache_block_size ，是key_buffer缓存块的单位长度，以字节为单位，默认值为1024。</p>
<p>key_cache_division_limit 控制着缓存块重用算法。默认值为100，此值为key_buffer_size中暖链所占的大小百分比(其中有暖链和热链)，100意味着全是暖链。(类似于Oracle Data Buffer Cache中的default、keep、recycle)</p>
<p>key_cache_age_threshold 如果key_buffer里的热链里的某个缓存块在这个变量所设定的时间里没有被访问过，MySQL服务器就会把它调整到暖链里去。这个参数值越大，缓存块在热链里停留的时间就越长。</p>
<p>这个参数默认值为 300，最小值为100。</p>
<p>Myisam索引默认是缓存在原始key_buffer中的，我们可以手动创建新的key_buffer，如在my.cnf中加入参数new_cache.key_buffer_size=20M。指定将table1和table2的索引缓存到new_cache的key_buffer中：</p>
<p>cache index table1,table2 in new_cache;</p>
<p>(之前默认的key_buffer为default，现在手动创建的为new_cache)</p>
<p>手动将table1和table2的索引载入到key_buffer中：</p>
<p>load index into cache table1,table2;</p>
<p>系统中记录的与Key Cache相关的性能状态参数变量： global status </p>
<p>l Key_blocks_not_flushed，已经更改但还未刷新到磁盘的DirtyCacheBlock； </p>
<p>l Key_blocks_unused，目前未被使用的CacheBlock数目； </p>
<p>l Key_blocks_used，已经使用了的CacheBlock数目； </p>
<p>l Key_read_requests，CacheBlock被请求读取的总次数； </p>
<p>l Key_reads，在CacheBlock中找不到需要读取的Key信息后到“.MYI”文件中(磁盘)读取的次数； </p>
<p>l Key_write_requests，CacheBlock被请求修改的总次数； </p>
<p>l Key_writes，在CacheBlock中找不到需要修改的Key信息后到“.MYI”文件中读入再修改的次数；</p>
<p>索引命中缓存率：</p>
<p>key_buffer_read_hits=(1-Key_reads/Key_read_requests)*100% </p>
<p>key_buffer_write_hits=(1-Key_writes/Key_write_requests)*100%</p>
<p>该命中率就代表了MyISAM类型表的索引的cache</p>
<h1 id="排序使用情况："><a href="#排序使用情况：" class="headerlink" title="排序使用情况："></a>排序使用情况：</h1><p>参数 ：sort_buffer_size - 单个thread能用来排序的内存空间大小，系统默认2M</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like &apos;sort%&apos;;</span><br><span class="line"></span><br><span class="line">+------------------+---------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+------------------+---------+</span><br><span class="line"></span><br><span class="line">| sort_buffer_size | 2097144 |</span><br><span class="line"></span><br><span class="line">+------------------+---------+</span><br><span class="line"></span><br><span class="line">mysql&gt; show global status like &apos;sort%&apos;;</span><br><span class="line"></span><br><span class="line">+-------------------+-------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+-------------------+-------+</span><br><span class="line"></span><br><span class="line">| Sort_merge_passes | 0 |#在内存中无法完成排序，而在磁盘上创建临时文件的次数(两倍)</span><br><span class="line"></span><br><span class="line">| Sort_range | 0 |#在范围内执行的排序的数量</span><br><span class="line"></span><br><span class="line">| Sort_rows | 0 |#已经排序的行数</span><br><span class="line"></span><br><span class="line">| Sort_scan | 0 |#通过扫描表完成的排序的数量</span><br><span class="line"></span><br><span class="line">+-------------------+-------+</span><br></pre></td></tr></table></figure>


<h1 id="表锁情况"><a href="#表锁情况" class="headerlink" title="表锁情况"></a>表锁情况</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show global status like &apos;table%&apos;;</span><br><span class="line"></span><br><span class="line">+-----------------------+-------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+-----------------------+-------+</span><br><span class="line"></span><br><span class="line">| Table_locks_immediate | 96 | # 表示立即释放的表锁数</span><br><span class="line"></span><br><span class="line">| Table_locks_waited | 0 | # 表示需要等待的表锁数</span><br><span class="line"></span><br><span class="line">+-----------------------+-------+</span><br></pre></td></tr></table></figure>
<p>如果 Table_locks_immediate / Table_locks_waited &gt; 5000，最好采用InnoDB引擎。</p>
<p>因为InnoDB是行锁而MyISAM是表锁，对于高并发写入的应用InnoDB效果会好些。</p>
<h1 id="表扫描情况"><a href="#表扫描情况" class="headerlink" title="表扫描情况"></a>表扫描情况</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show global status like &apos;handler_read%&apos;;</span><br><span class="line"></span><br><span class="line">+-----------------------+-------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+-----------------------+-------+</span><br><span class="line"></span><br><span class="line">| Handler_read_first | 60 |</span><br><span class="line"></span><br><span class="line">| Handler_read_key | 2442 |</span><br><span class="line"></span><br><span class="line">| Handler_read_next | 286 |</span><br><span class="line"></span><br><span class="line">| Handler_read_prev | 0 |</span><br><span class="line"></span><br><span class="line">| Handler_read_rnd | 28 |</span><br><span class="line"></span><br><span class="line">| Handler_read_rnd_next | 3191 |</span><br><span class="line"></span><br><span class="line">+-----------------------+-------+</span><br><span class="line"></span><br><span class="line">mysql&gt; show global status like &apos;com_select&apos;;</span><br><span class="line"></span><br><span class="line">+---------------+-------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+---------------+-------+</span><br><span class="line"></span><br><span class="line">| Com_select | 23 |</span><br><span class="line"></span><br><span class="line">+---------------+-------+</span><br></pre></td></tr></table></figure>
<p>计算表扫描率： </p>
<p>表扫描率 = Handler_read_rnd_next / Com_select </p>
<p>如果：</p>
<p>表扫描率超过4000，说明进行了太多表扫描，很有可能索引没有建好，</p>
<p>增加read_buffer_size值会有一些好处，但最好不要超过8MB。</p>
<h2 id="Handler-read-first"><a href="#Handler-read-first" class="headerlink" title="Handler_read_first"></a>Handler_read_first</h2><p>此选项表明SQL是在做一个全索引扫描(注意是全部，而不是部分)，所以说如果存在WHERE语句，</p>
<p>这个值是不会变的。如果这个值的数值很大，既是好事 也是坏事。</p>
<p>说它好是因为毕竟查询是在索引里完成的，而不是数据文件里，说它坏是因为大数据量时，</p>
<p>即便是索引文件，做一次完整的扫描也是很费时的。</p>
<h2 id="Handler-read-key"><a href="#Handler-read-key" class="headerlink" title="Handler_read_key"></a>Handler_read_key</h2><p>此选项数值如果很高，说明系统高效的使用了索引，一切运转良好</p>
<h1 id="Handler-read-next"><a href="#Handler-read-next" class="headerlink" title="Handler_read_next"></a>Handler_read_next</h1><p>此选项表明在进行索引扫描时，按照索引从数据文件里取数据的次数</p>
<h2 id="Handler-read-prev"><a href="#Handler-read-prev" class="headerlink" title="Handler_read_prev"></a>Handler_read_prev</h2><p>此选项表明在进行索引扫描时，按照索引倒序从数据文件里取数据的次数，</p>
<p>一般就是ORDER BY … DESC。</p>
<h2 id="Handler-read-rnd"><a href="#Handler-read-rnd" class="headerlink" title="Handler_read_rnd"></a>Handler_read_rnd</h2><p>简单的说，就是查询直接操作了数据文件，很多时候表现为没有使用索引或者文件排序</p>
<p>可能是有大量的全表扫描或连接时没恰当使用keys。</p>
<p>The number of requests to read a row based on a fixed position. This value is </p>
<p>high if you are doing a lot of queries that require sorting of the result. You </p>
<p>probably have a lot of queries that require MySQL to scan entire tables or you </p>
<p>have joins that do not use keys properly.</p>
<h2 id="Handler-read-rnd-next"><a href="#Handler-read-rnd-next" class="headerlink" title="Handler_read_rnd_next"></a>Handler_read_rnd_next</h2><p>此选项表明在进行数据文件扫描时，从数据文件里取数据的次数。(物理IO次数)</p>
<h1 id="MySQL内存分配"><a href="#MySQL内存分配" class="headerlink" title="MySQL内存分配"></a>MySQL内存分配</h1><p>mysql服务器为每个连接上的客户端线程，分配的内存空间：</p>
<p>read_buffer_size + read_rnd_buffer_size + sort_buffer_size + </p>
<p>thread_stack + join_buffer_size</p>
<p>从内存的使用方式MySQL 数据库的内存使用主要分为以下两类</p>
<p>· 线程独享内存</p>
<p>· 全局共享内存</p>
<p>先分析 MySQL 中主要的 “线程独享内存” 的。</p>
<p>在 MySQL 中，线程独享内存主要用于各客户端连接线程存储各种操作的独享数据，如线程栈信息，分组排序操作，数据读写缓冲，结果集暂存等等，而且大多数可以通过相关参数来控制内存的使用量。</p>
<p>线程栈信息使用内存(thread_stack)：</p>
<p>主要用来存放每一个线程自身的标识信息，如线程id，线程运行时基本信息等等，我们可以通过 thread_stack 参数来设置为每一个线程栈分配多大的内存。</p>
<p>排序使用内存(sort_buffer_size)：</p>
<p>MySQL 用此内存区域进行排序操作（filesort），完成客户端的排序请求。当我们设置的排序区缓存大小无法满足排序实际所需内存的时候，MySQL 会将数据写入磁盘文件来完成排序。由于磁盘和内存的读写性能完全不在一个数量级，所以sort_buffer_size参数对排序操作的性能影响绝对不可小视。排序操作的实现原理请参考：MySQL Order By 的实现分析(<a href="http://www.kuqin.com/database/20081206/29716.html)。" target="_blank" rel="noopener">http://www.kuqin.com/database/20081206/29716.html)。</a></p>
<p>Join操作使用内存(join_buffer_size)：</p>
<p>应用程序经常会出现一些两表（或多表）Join的操作需求，MySQL在完成某些 Join 需求的时候（all/index join），为了减少参与Join的“被驱动表”的读取次数以提高性能，需要使用到 Join Buffer 来协助完成 Join操作（具体 Join 实现算法请参考：</p>
<p>MySQL 中的 Join 基本实现原理(<a href="http://www.kuqin.com/database/20081206/29717.html)）。当" target="_blank" rel="noopener">http://www.kuqin.com/database/20081206/29717.html)）。当</a> Join Buffer 太小，MySQL 不会将该 Buffer 存入磁盘文件，而是先将Join Buffer中的结果集与需要 Join 的表进行 Join 操作，然后清空 Join Buffer 中的数据，继续将剩余的结果集写入此 Buffer 中，如此往复。这势必会造成被驱动表需要被多次读取，成倍增加 IO 访问，降低效率。</p>
<p>顺序读取数据缓冲区使用内存(read_buffer_size)：</p>
<p>这部分内存主要用于当需要顺序读取数据的时候，如无法使用索引的情况下的全表扫描，全索引扫描等。在这种时候，MySQL 按照数据的存储顺序依次读取数据块，每次读取的数据快首先会暂存在read_buffer_size中，当 buffer 空间被写满或者全部数据读取结束后，再将buffer中的数据返回给上层调用者，以提高效率。</p>
<p>随机读取数据缓冲区使用内存(read_rnd_buffer_size)：</p>
<p>和顺序读取相对应，当 MySQL 进行非顺序读取（随机读取）数据块的时候，会利用这个缓冲区暂存读取的数据。如根据索引信息读取表数据，根据排序后的结果集与表进行Join等等。总的来说，就是当数据块的读取需要满足一定的顺序的情况下，MySQL 就需要产生随机读取，进而使用到 read_rnd_buffer_size 参数所设置的内存缓冲区。</p>
<p>连接信息及返回客户端前结果集暂存使用内存(net_buffer_size)：</p>
<p>这部分用来存放客户端连接线程的连接信息和返回客户端的结果集。当 MySQL 开始产生可以返回的结果集，会在通过网络返回给客户端请求线程之前，会先暂存在通过 net_buffer_size 所设置的缓冲区中，等满足一定大小的时候才开始向客户端发送，以提高网络传输效率。不过，net_buffer_size 参数所设置的仅仅只是该缓存区的初始化大小，MySQL 会根据实际需要自行申请更多的内存以满足需求，但最大不会超过 max_allowed_packet 参数大小。</p>
<p>批量插入暂存使用内存(bulk_insert_buffer_size)：</p>
<p>当我们使用如 insert … values(…),(…),(…)… 的方式进行批量插入的时候，MySQL 会先将提交的数据放如一个缓存空间中，当该缓存空间被写满或者提交完所有数据之后，MySQL 才会一次性将该缓存空间中的数据写入数据库并清空缓存。此外，当我们进行 LOAD DATA INFILE 操作来将文本文件中的数据 Load 进数据库的时候，同样会使用到此缓冲区。</p>
<p>MySQL对硬件的”收益递减点“为256G内存，32CPU。</p>
<h1 id="dalayed-queue-size"><a href="#dalayed-queue-size" class="headerlink" title="dalayed_queue_size"></a>dalayed_queue_size</h1><p>在被插入到实际的数据表里之前，来自insert delayed语句的数据航将在每个队列里等待</p>
<p>MySQL来处理他们。delayed_queue_size就是这个队列所能容纳的数据航的最大个数。当</p>
<p>这个队列满是，后续的insert delayed语句将被阻塞，直到这个队列里有容纳他们的空间</p>
<p>为止。</p>
<p>如果有很多客户在发出insert delayed语句以避免受阻塞，但你发现这些语句有阻塞的迹象，</p>
<p>加大这个变量的值将使更多的insert delayed语句更快地得到处理。</p>
<h1 id="文件打开数-open-files-limit"><a href="#文件打开数-open-files-limit" class="headerlink" title="文件打开数 open_files_limit"></a>文件打开数 open_files_limit</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like &apos;open%&apos;;</span><br><span class="line"></span><br><span class="line">+------------------+-------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+------------------+-------+</span><br><span class="line"></span><br><span class="line">| open_files_limit | 1024 | #mysql总共能够打开的文件的数量</span><br><span class="line"></span><br><span class="line">+------------------+-------+</span><br><span class="line"></span><br><span class="line">mysql&gt; show global status like &apos;open%file%&apos;;</span><br><span class="line"></span><br><span class="line">+---------------+-------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+---------------+-------+</span><br><span class="line"></span><br><span class="line">| Open_files | 79 | # 系统当前打开的文件数</span><br><span class="line"></span><br><span class="line">| Opened_files | 278 | # 系统打开过的文件总数</span><br><span class="line"></span><br><span class="line">+---------------+-------+</span><br></pre></td></tr></table></figure>
<p>比较合适的设置：Open_files / open_files_limit * 100% &lt;= 75%</p>
]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>为何选择logback+slf4j</title>
    <url>/post/e2c1fb46/</url>
    <content><![CDATA[<p>无论从设计上还是实现上，Logback相对log4j而言有了相对多的改进。不过尽管难以一一细数，这里还是列举部分理由为什么选择logback而不是log4j。牢记logback与log4j在概念上面是很相似的，它们都是有同一群开发者建立。所以如果你已经对log4j很熟悉，你也可以很快上手logback。如果你喜欢使用log4j,你也许会迷上使用logback。</p>
<ol>
<li>更快的执行速度</li>
</ol>
<p>基于我们先前在log4j上的工作，logback 重写了内部的实现，在某些特定的场景上面，甚至可以比之前的速度快上10倍。在保证logback的组件更加快速的同时，同时所需的内存更加少。</p>
<p>2.更充足的测试</p>
<p>Logback 历经了几年，数不清小时数的测试。尽管log4j也是测试过的，但是Logback的测试更加充分，跟log4j不在同一个级别。我们认为，这正是人们选择Logback而不是log4j的最重要的原因。人们都希望即使在恶劣的条件下，你的日记框架依然稳定而可靠。</p>
<ol start="3">
<li>logback-classic 非常自然的实现了SLF4J</li>
</ol>
<p>logback-classic中的登陆类自然的实现了SLF4J。当你使用 logback-classic作为底层实现时，涉及到LF4J日记系统的问题你完全不需要考虑。更进一步来说，由于 logback-classic强烈建议使用SLF4J作为客户端日记系统实现，如果需要切换到log4j或者其他，你只需要替换一个jar包即可，不需要去改变那些通过SLF4J API 实现的代码。这可以大大减少更换日记系统的工作量。</p>
<ol start="4">
<li>扩展文档</li>
</ol>
<p>Logback附带详细的和不断更新的文档。</p>
<p>5.使用XML配置文件或者Groovy</p>
<p>配置logback的传统方法是通过XML文件。在文档中，大部分例子都是是用XML语法。但是，对于logback版本0.9.22，通过Groovy编写的配置文件也得到支持。相比于XML，Groovy风格的配置文件更加直观，连贯和简短的语法。<br>现在， 已经有一个工具自动把logback.xml文件迁移至logback.groovy。</p>
<p>6.自动重新载入配置文件</p>
<p>Logback-classic可以在配置文件被修改后，自动重新载入。这个扫描过程很快，无资源争用，并且可以动态扩展支持在上百个线程之间每秒上百万个调用。它和应用服务器结合良好，并且在JEE环境通用，因为它不会调用创建一个单独的线程来做扫描。</p>
<p>7.优雅地从I/O错误中恢复</p>
<p>FileAppender和它的子类，包括RollingFileAppender，可以优雅的从I/O错误中恢复。所以，如果一个文件服务器临时宕机，你再也不需要重启你的应用，而日志功能就能正常工作。当文件服务器恢复工作，logback相关的appender就会透明地和快速的从上一个错误中恢复。</p>
<p>8.自动清除旧的日志归档文件</p>
<p>通过设置TimeBasedRollingPolicy 或者 SizeAndTimeBasedFNATP的 maxHistory 属性，你就可以控制日志归档文件的最大数量。如果你的回滚策略是每月回滚的，并且你希望保存一年的日志，那么只需简单的设置maxHistory属性为12。对于12个月之前的归档日志文件将被自动清除。</p>
<ol start="9">
<li>自动压缩归档日志文件</li>
</ol>
<p>RollingFileAppender可以在回滚操作中，自动压缩归档日志文件。压缩通常是异步执行的，所以即使是很大的日志文件，你的应用都不会因此而被阻塞。</p>
<ol start="10">
<li>谨慎模式</li>
</ol>
<p>在谨慎模式中，在多个JVM中运行的多个FileAppender实例，可以安全的写入统一个日志文件。谨慎模式可以在一定的限制条件下应用于RollingFileAppender。</p>
<p>11.Lilith</p>
<p>Lilith是logback的一个记录和访问事件查看器。它相当于log4j的 chainsaw，但是Lilith设计的目的是处理大量的日志记录。</p>
<ol start="12">
<li>配置文件中的条件处理</li>
</ol>
<p>开发者通常需要在不同的目标环境中变换logback的配置文件，例如开发环境，测试环境和生产环境。这些配置文件大体是一样的，除了某部分会有不同。为了避免重复，logback支持配置文件中的条件处理，只需使用<if>,<then>和<else>，那么同一个配置文件就可以在不同的环境中使用了。</else></then></if></p>
<p>13.过滤</p>
<p>Logback拥有远比log4j更丰富的过滤能力。例如，让我们假设，有一个相当重要的商业应用部署在生产环境。考虑到大量的交易数据需要处理，记录级别被设置为WARN，那么只有警告和错误信息才会被记录。现在，想象一下，你在开发环境遇到了一个臭虫，但是在测试平台中却很难发现，因为一些环境之间(生产环境/测试环境)的未知差异。</p>
<p>使用log4j，你只能选择在生产系统中降低记录的级别到DEBUG，来尝试发现问题。但是很不幸，这会生成大量的日志记录，让分析变得困难。更重要的是，多余的日志记录会影响到生产环境的性能。<br>使用logback，你可以选择保留只所有用户的WARN级别的日志，而除了某个用户，例如Alice，而她就是问题的相关用户。当Alice登录系统，她就会以DEBUG级别被记录，而其他用户仍然是以WARN级别来记录日志。这个功能，可以通过在配置文件的XML中添加4行。请在相关章节中查找MDCFilter</p>
<ol start="14">
<li>SiftingAppender</li>
</ol>
<p>SiftingAppender是一个全能的追加器。它可以基于任何给定的实时属性分开（或者筛选）日志。例如，SiftingAppender可以基于用户会话分开日志事件，这样，可以为每一个用户建立一个独立的日志文件。</p>
<p>15.Logback-access模块，提供了通过HTTP访问日志的能力，是logback不可或缺的组成部分</p>
<p>最后但绝非最不重要的是，作为logback发布包的一部分，logback-access模块可与Jetty或者Tomcat进行集成，提供了非常丰富而强大的通过HTTP访问日志的功能。因为logback-access模块是logback初期设计方案中的一部分，因此，所有你所喜欢的logback-classic模块所提供的全部特性logback-access同样也具备。</p>
<p>以上网络整理的内容————————————————————–<br>下面是我编写的一个demo配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration scan=&quot;false&quot; debug=&quot;false&quot;&gt;  </span><br><span class="line">  	&lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;</span><br><span class="line">        &lt;encoder&gt;</span><br><span class="line">            &lt;pattern&gt;%d&#123;mm:ss&#125; %-5level %logger&#123;36&#125; &gt;&gt;&gt; %msg%n&lt;/pattern&gt;</span><br><span class="line">        &lt;/encoder&gt;</span><br><span class="line">    &lt;/appender&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;appender name=&quot;DEBUG_FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class="line">    	&lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;  </span><br><span class="line">            &lt;level&gt;DEBUG&lt;/level&gt;</span><br><span class="line">            &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;  </span><br><span class="line">    		&lt;onMismatch&gt;DENY&lt;/onMismatch&gt;</span><br><span class="line">        &lt;/filter&gt; </span><br><span class="line">	    &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class="line">	      &lt;!-- daily rollover --&gt;</span><br><span class="line">	      &lt;fileNamePattern&gt;logs/debug.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt;</span><br><span class="line">	</span><br><span class="line">	      &lt;!-- keep 30 days&apos; worth of history capped at 3GB total size --&gt;</span><br><span class="line">	      &lt;maxHistory&gt;30&lt;/maxHistory&gt;</span><br><span class="line">	      &lt;totalSizeCap&gt;3GB&lt;/totalSizeCap&gt;</span><br><span class="line">	    &lt;/rollingPolicy&gt;</span><br><span class="line">	    &lt;encoder&gt;</span><br><span class="line">	      &lt;pattern&gt;%d [%thread] [%-5level] %file,%line - %msg%n&lt;/pattern&gt;</span><br><span class="line">	    &lt;/encoder&gt; </span><br><span class="line">    &lt;/appender&gt;  </span><br><span class="line">  </span><br><span class="line">    &lt;appender name=&quot;WARN_FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;</span><br><span class="line">    	&lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;  </span><br><span class="line">            &lt;level&gt;WARN&lt;/level&gt;</span><br><span class="line">        &lt;/filter&gt; </span><br><span class="line">	    &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;</span><br><span class="line">	      &lt;!-- daily rollover --&gt;</span><br><span class="line">	      &lt;fileNamePattern&gt;logs/warn.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt;</span><br><span class="line">	</span><br><span class="line">	      &lt;!-- keep 30 days&apos; worth of history capped at 3GB total size --&gt;</span><br><span class="line">	      &lt;maxHistory&gt;30&lt;/maxHistory&gt;</span><br><span class="line">	      &lt;totalSizeCap&gt;3GB&lt;/totalSizeCap&gt;</span><br><span class="line">	    &lt;/rollingPolicy&gt;</span><br><span class="line">	    &lt;encoder&gt;</span><br><span class="line">	      &lt;pattern&gt;%d [%thread] [%-5level] %file,%line - %msg%n&lt;/pattern&gt;</span><br><span class="line">	    &lt;/encoder&gt; </span><br><span class="line">    &lt;/appender&gt; </span><br><span class="line">  </span><br><span class="line">    &lt;root level=&quot;DEBUG&quot;&gt;  </span><br><span class="line">        &lt;appender-ref ref=&quot;DEBUG_FILE&quot; /&gt;  </span><br><span class="line">        &lt;appender-ref ref=&quot;WARN_FILE&quot; /&gt;  </span><br><span class="line">        &lt;appender-ref ref=&quot;STDOUT&quot; /&gt;</span><br><span class="line">    &lt;/root&gt;  </span><br><span class="line">  </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class TestA &#123;</span><br><span class="line">	static Logger log = LoggerFactory.getLogger(TestB.class);</span><br><span class="line">	</span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		log.trace(&quot;======trace&quot;);  </span><br><span class="line">		log.debug(&quot;======debug&quot;);  </span><br><span class="line">		log.info(&quot;======info&quot;);  </span><br><span class="line">		log.warn(&quot;======warn&quot;);  </span><br><span class="line">		log.error(&quot;======error&quot;); </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出结果:</p>
<p><img data-src="/images/loading.png" data-original="/images/pasted-1.png" alt="upload successful"></p>
<p>日志记录情况:</p>
<p><img data-src="/images/loading.png" data-original="/images/pasted-2.png" alt="upload successful"></p>
<p>demo每天一个新日志文件,历史数据保留最大30天.日志总占数据量大于3G自动异步删除旧日志</p>
<p>根节点<code>&lt;configuration&gt;</code>包含的属性：</p>
<p>scan:</p>
<p>当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。</p>
<p>scanPeriod:</p>
<p>设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。</p>
<p>debug:</p>
<p>当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。</p>
<p><code>&lt;level&gt;</code>:设置过滤级别</p>
<p><code>&lt;onMatch&gt;</code>:用于配置符合过滤条件的操作</p>
<p><code>&lt;onMismatch&gt;</code>:用于配置不符合过滤条件的操作</p>
<p>Logback的过滤器基于三值逻辑（ternary logic），允许把它们组装或成链，从而组成任意的复合过滤策略。过滤器很大程度上受到Linux的iptables启发。这里的所谓三值逻辑是说，过滤器的返回值只能是ACCEPT、DENY和NEUTRAL的其中一个。</p>
<p>如果返回DENY，那么记录事件立即被抛弃，不再经过剩余过滤器；</p>
<p>如果返回NEUTRAL，那么有序列表里的下一个过滤器会接着处理记录事件；</p>
<p>如果返回ACCEPT，那么记录事件被立即处理，不再经过剩余过滤器。</p>
<p><code>&lt;maxHistory&gt;</code>:<br>可选节点，控制保留的归档文件的最大数量，超出数量就删除旧文件。假设设置每个月滚动，且<maxHistory>是6，则只保存最近6个月的文件，删除之前的旧文件。注意，删除旧文件是，那些为了归档而创建的目录也会被删除。</maxHistory></p>
<p>独立的日志标签</p>
<p><code>&lt;logger name=&quot;monitor&quot; additivity=&quot;false&quot; level=&quot;INFO&quot;&gt;</code></p>
<p>这里通过设置additivity=”false”禁止monitor里的内容向上传递，否则会同时显示在默认的日志中。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>logback</tag>
        <tag>slf4j</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title>dubbo学习记录</title>
    <url>/post/33b13b3d/</url>
    <content><![CDATA[<h2 id="什么是RPC"><a href="#什么是RPC" class="headerlink" title="什么是RPC"></a>什么是RPC</h2><p>分布式计算，远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一个地址空间（通常为一个开放网络的一台计算机）的子程序，而程序员就像调用本地程序一样，无需额外地为这个交互作用编程（无需关注细节）。RPC是一种服务器-客户端（Client/Server）模式，经典实现是一个通过发送请求-接受回应进行信息交互的系统。</p>
<p>如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用，例：Java RMI。</p>
<p>RPC是一种进程间通信的模式，程序分布在不同的地址空间里。如果在同一主机里，RPC可以通过不同的虚拟地址空间（即便使用相同的物理地址）进行通讯，而在不同的主机间，则通过不同的物理进行交互。许多技术（常常是不兼容）都是基于这种概念而实现的。</p>
<h2 id="RPC实现原理"><a href="#RPC实现原理" class="headerlink" title="RPC实现原理"></a>RPC实现原理</h2><p>首先需要有处理网络连接通讯的模块，负责连接建立、管理和消息的传输。其次需要有编<br>解码的模块，因为网络通讯都是传输的字节码，需要将我们使用的对象序列化和反序列<br>化。剩下的就是客户端和服务器端的部分，服务器端暴露要开放的服务接口，客户调用服<br>务接口的一个代理实现，这个代理实现负责收集数据、编码并传输给服务器然后等待结果<br>返回。</p>
<h2 id="Dubbo的实现原理"><a href="#Dubbo的实现原理" class="headerlink" title="Dubbo的实现原理"></a>Dubbo的实现原理</h2><p>dubbo 作为 rpc 框架，实现的效果就是调用远程的方法就像在本地调用一样。如何做到<br>呢？就是本地有对远程方法的描述，包括方法名、参数、返回值，在 dubbo 中是远程和本<br>地使用同样的接口；然后呢，要有对网络通信的封装，要对调用方来说通信细节是完全不<br>可见的，网络通信要做的就是将调用方法的属性通过一定的协议（简单来说就是消息格<br>式）传递到服务端；服务端按照协议解析出调用的信息；执行相应的方法；在将方法的返<br>回值通过协议传递给客户端；客户端再解析；在调用方式上又可以分为同步调用和异步调<br>用；简单来说基本就这个过程</p>
<h2 id><a href="#" class="headerlink" title></a></h2>]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title>快速生成Maven archetype模板</title>
    <url>/post/e0f55110/</url>
    <content><![CDATA[<h2 id="快捷生成archetype方便修改"><a href="#快捷生成archetype方便修改" class="headerlink" title="快捷生成archetype方便修改"></a>快捷生成archetype方便修改</h2><p>mvn archetype:create-from-project -Darchetype.filteredExtensions=java -DpackageName=com.pearlgo -DartifactId=ec-template</p>
<h3 id="上传archetype到私服"><a href="#上传archetype到私服" class="headerlink" title="上传archetype到私服"></a>上传archetype到私服</h3><p>添加配置pom配置,是配置到archetype的pom</p>
<!-- 配置部署的远程仓库 -->
<p> <code>&lt;distributionManagement&gt;
  &lt;snapshotRepository&gt;
   &lt;id&gt;nexus-snapshots&lt;/id&gt;
   &lt;name&gt;nexus distribution snapshot repository&lt;/name&gt;
   &lt;url&gt;xxxx私服snapshots地址&lt;/url&gt;
  &lt;/snapshotRepository&gt;
  &lt;repository&gt;
   &lt;id&gt;nexus-releases&lt;/id&gt;
   &lt;name&gt;Nexus Release Repository&lt;/name&gt;
   &lt;url&gt;xxxx私服release地址&lt;/url&gt;
  &lt;/repository&gt;
 &lt;/distributionManagement&gt;</code></p>
<h2 id="执行上传-用IDEA可以直接用快捷方法无需用命令"><a href="#执行上传-用IDEA可以直接用快捷方法无需用命令" class="headerlink" title="执行上传,用IDEA可以直接用快捷方法无需用命令"></a>执行上传,用IDEA可以直接用快捷方法无需用命令</h2><p>mvn clean deploy</p>
]]></content>
      <categories>
        <category>archetype</category>
        <category>maven</category>
      </categories>
      <tags>
        <tag>archetype</tag>
        <tag>maven</tag>
        <tag>快捷</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux操作</title>
    <url>/post/7a00f2af/</url>
    <content><![CDATA[<h3 id="找到根目录下所有的以test开头的文件并把查找结果当做参数传给rm-rf命令进行删除："><a href="#找到根目录下所有的以test开头的文件并把查找结果当做参数传给rm-rf命令进行删除：" class="headerlink" title="找到根目录下所有的以test开头的文件并把查找结果当做参数传给rm -rf命令进行删除："></a>找到根目录下所有的以test开头的文件并把查找结果当做参数传给rm -rf命令进行删除：</h3><p>1、find / -name “test<em>” |xargs rm -rf<br>2、find / -name “test</em>” -exec rm -rf {} ;<br>3、rm -rf $(find / -name “test”)</p>
<h3 id="如果想指定递归深度，可以这样："><a href="#如果想指定递归深度，可以这样：" class="headerlink" title="如果想指定递归深度，可以这样："></a>如果想指定递归深度，可以这样：</h3><p>1、find / -maxdepth 3 -name “<em>.mp3” |xargs rm -rf<br>2、find / -maxdepth 3 -name “test</em>” -exec rm -rf {} ;<br>3、rm -rf $(find / -maxdepth 3 -name “test”)<br>这样只会查找三层目录中符合条件的文件并删除掉！ </p>
<h3 id="使用特定用户重启"><a href="#使用特定用户重启" class="headerlink" title="使用特定用户重启"></a>使用特定用户重启</h3><p>sudo -u apprunner bash ./start.sh restart</p>
<p>sudo -u apprunner /usr/local/java/jdk1.8.0_152/bin</p>
<h3 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h3><p>ps aux –sort -rss|grep java</p>
<h4 id="线程数等数据"><a href="#线程数等数据" class="headerlink" title="线程数等数据"></a>线程数等数据</h4><p>cat /proc/PID/status</p>
<h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><p>tar -czvf 文件名 要打包内容</p>
<h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p>tar -xzvf file.tar.gz</p>
<h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>zcat vsftpd.tar.gz|grep –binary-files=text ‘footbar.js’或<br>zgrep –binary-files=text ‘footbar.js’ vsftpd.tar.gz</p>
<p>sudo rz -y 覆盖文件<br>sudo unzip -o<br>sudo cp -rf<br>chmod +x *.sh</p>
<p>df -lh 磁盘大小</p>
<p>du -h –max-depth=1 文件夹大小</p>
<p>sudo scp -P 58880  ./dwc-pj.tar.gz <a href="mailto:dwc-lyp@112.74.140.11" target="_blank" rel="noopener">dwc-lyp@112.74.140.11</a>:/data/project/dwc</p>
<p>jmap -histo:live 15441|less</p>
<p>netstat -nap|grep 120.24.166.239:6379</p>
<p>nohup java …. &gt; xx.txt &amp;</p>
<p>jps</p>
<p>功能</p>
<p>显示当前所有java进程pid的命令。</p>
<p>常用指令</p>
<p>jps：显示当前用户的所有java进程的PID<br>jps -v 3331：显示虚拟机参数<br>jps -m 3331：显示传递给main()函数的参数<br>jps -l 3331：显示主类的全路径<br>详细介绍</p>
<p>jinfo</p>
<p>功能</p>
<p>实时查看和调整虚拟机参数，可以显示未被显示指定的参数的默认值（jps -v 则不能）。</p>
<p>jdk8中已经不支持该命令。</p>
<p>常用指令</p>
<p>jinfo -flag CMSIniniatingOccupancyFration 1444：查询CMSIniniatingOccupancyFration参数值</p>
<p>详细介绍</p>
<p>jstat</p>
<p>功能</p>
<p>显示进程中的类装载、内存、垃圾收集、JIT编译等运行数据。</p>
<p>常用指令</p>
<p>jstat -gc 3331 250 20 ：查询进程2764的垃圾收集情况，每250毫秒查询一次，一共查询20次。<br>jstat -gccause：额外输出上次GC原因<br>jstat -calss：件事类装载、类卸载、总空间以及所消耗的时间<br>详细介绍</p>
<p>jmap</p>
<p>功能</p>
<p>生成堆转储快照（heapdump）</p>
<p>常用指令</p>
<p>jmap -heap 3331：查看java 堆（heap）使用情况<br>jmap -histo 3331：查看堆内存(histogram)中的对象数量及大小<br>jmap -histo:live 3331：JVM会先触发gc，然后再统计信息<br>jmap -dump:format=b,file=heapDump 3331：将内存使用的详细情况输出到文件，之后一般使用其他工具进行分析。<br>详细介绍</p>
<p>jhat</p>
<p>功能</p>
<p>一般与jmap搭配使用，用来分析jmap生成的堆转储文件。</p>
<p>由于有很多可视化工具（Eclipse Memory Analyzer 、IBM HeapAnalyzer）可以替代，所以很少用。不过在没有可视化工具的机器上也是可用的。</p>
<p>常用指令</p>
<p>jmap -dump:format=b,file=heapDump 3331 + jhat heapDump：解析Java堆转储文件,并启动一个 web server</p>
<p>详细介绍</p>
<p>jstack</p>
<p>功能</p>
<p>生成当前时刻的线程快照。</p>
<p>常用指令</p>
<p>jstack 3331：查看线程情况<br>jstack -F 3331：正常输出不被响应时，使用该指令<br>jstack -l 3331：除堆栈外，显示关于锁的附件信息<br>详细介绍</p>
<p>常见问题定位过程</p>
<p>频繁GC问题或内存溢出问题</p>
<p>一、使用jps查看线程ID</p>
<p>二、使用jstat -gc 3331 250 20 查看gc情况，一般比较关注PERM区的情况，查看GC的增长情况。</p>
<p>三、使用jstat -gccause：额外输出上次GC原因</p>
<p>四、使用jmap -dump:format=b,file=heapDump 3331生成堆转储文件</p>
<p>五、使用jhat或者可视化工具（Eclipse Memory Analyzer 、IBM HeapAnalyzer）分析堆情况。</p>
<p>六、结合代码解决内存溢出或泄露问题。</p>
<p>死锁问题</p>
<p>一、使用jps查看线程ID</p>
<p>二、使用jstack 3331：查看线程情况</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>IDEA常用快捷键</title>
    <url>/post/566321e7/</url>
    <content><![CDATA[<h2 id="记录原因"><a href="#记录原因" class="headerlink" title="记录原因"></a>记录原因</h2><p>因为经常忘记快捷键,所以写一遍记录常用快捷键的博客,方便忘记的时候可以快速查找,不用到处翻。</p>
<a id="more"></a>
<h2 id="常用"><a href="#常用" class="headerlink" title="常用"></a>常用</h2><p>Ctrl+Shift + Enter，语句完成<br>“！”，否定完成，输入表达式时按 “！”键<br>Ctrl+E，最近的文件<br>Ctrl+Shift+E，最近更改的文件<br>Shift+Click，可以关闭文件<br>Ctrl+[ OR ]，可以跑到大括号的开头与结尾<br>Ctrl+F12，可以显示当前文件的结构<br>Ctrl+F7，可以查询当前元素在当前文件中的引用，然后按 F3 可以选择<br>Ctrl+N，可以快速打开类<br>Ctrl+Shift+N，可以快速打开文件<br>Alt+Q，可以看到当前方法的声明<br>Ctrl+P，可以显示参数信息<br>Ctrl+Shift+Insert，可以选择剪贴板内容并插入<br>Alt+Insert，可以生成构造器/Getter/Setter等<br>Ctrl+Alt+V，可以引入变量。例如：new String(); 自动导入变量定义<br>Ctrl+Alt+T，可以把代码包在一个块内，例如：try/catch<br>Ctrl+Enter，导入包，自动修正<br>Ctrl+Alt+L，格式化代码<br>Ctrl+Alt+I，将选中的代码进行自动缩进编排，这个功能在编辑 JSP 文件时也可以工作<br>Ctrl+Alt+O，优化导入的类和包<br>Ctrl+R，替换文本<br>Ctrl+F，查找文本<br>Ctrl+Shift+Space，自动补全代码<br>Ctrl+空格，代码提示（与系统输入法快捷键冲突）<br>Ctrl+Shift+Alt+N，查找类中的方法或变量<br>Alt+Shift+C，最近的更改<br>Alt+Shift+Up/Down，上/下移一行<br>Shift+F6，重构 – 重命名<br>Ctrl+X，删除行<br>Ctrl+D，复制行<br>Ctrl+/或Ctrl+Shift+/，注释（//或者/**/）<br>Ctrl+J，自动代码（例如：serr）<br>Ctrl+Alt+J，用动态模板环绕<br>Ctrl+H，显示类结构图（类的继承层次）<br>Ctrl+Q，显示注释文档<br>Alt+F1，查找代码所在位置<br>Alt+1，快速打开或隐藏工程面板<br>Ctrl+Alt+left/right，返回至上次浏览的位置<br>Alt+left/right，切换代码视图<br>Alt+Up/Down，在方法间快速移动定位<br>Ctrl+Shift+Up/Down，向上/下移动语句<br>F2 或 Shift+F2，高亮错误或警告快速定位<br>Tab，代码标签输入完成后，按 Tab，生成代码<br>Ctrl+Shift+F7，高亮显示所有该文本，按 Esc 高亮消失<br>Alt+F3，逐个往下查找相同文本，并高亮显示<br>Ctrl+Up/Down，光标中转到第一行或最后一行下<br>Ctrl+B/Ctrl+Click，快速打开光标处的类或方法（跳转到定义处）<br>Ctrl+Alt+B，跳转到方法实现处<br>Ctrl+Shift+Backspace，跳转到上次编辑的地方<br>Ctrl+O，重写方法<br>Ctrl+Alt+Space，类名自动完成<br>Ctrl+Alt+Up/Down，快速跳转搜索结果<br>Ctrl+Shift+J，整合两行<br>Alt+F8，计算变量值<br>Ctrl+Shift+V，可以将最近使用的剪贴板内容选择插入到文本<br>Ctrl+Alt+Shift+V，简单粘贴<br>Shift+Esc，不仅可以把焦点移到编辑器上，而且还可以隐藏当前（或最后活动的）工具窗口<br>F12，把焦点从编辑器移到最近使用的工具窗口<br>Shift+F1，要打开编辑器光标字符处使用的类或者方法 Java 文档的浏览器<br>Ctrl+W，可以选择单词继而语句继而行继而函数<br>Ctrl+Shift+W，取消选择光标所在词<br>Alt+F7，查找整个工程中使用地某一个类、方法或者变量的位置<br>Ctrl+I，实现方法<br>Ctrl+Shift+U，大小写转化<br>Ctrl+Y，删除当前行<br>Shift+Enter，向下插入新行<br>psvm/sout，main/System.out.println(); Ctrl+J，查看更多<br>Ctrl+Shift+F，全局查找<br>Ctrl+F，查找/Shift+F3，向上查找/F3，向下查找<br>Ctrl+Shift+S，高级搜索<br>Ctrl+U，转到父类<br>Ctrl+Alt+S，打开设置对话框<br>Alt+Shift+Inert，开启/关闭列选择模式<br>Ctrl+Alt+Shift+S，打开当前项目/模块属性<br>Ctrl+G，定位行<br>Alt+Home，跳转到导航栏<br>Ctrl+Enter，上插一行<br>Ctrl+Backspace，按单词删除<br>Ctrl+”+/-”，当前方法展开、折叠<br>Ctrl+Shift+”+/-”，全部展开、折叠</p>
<h2 id="【调试部分、编译】"><a href="#【调试部分、编译】" class="headerlink" title="【调试部分、编译】"></a>【调试部分、编译】</h2><p>Ctrl+F2，停止<br>Alt+Shift+F9，选择 Debug<br>Alt+Shift+F10，选择 Run<br>Ctrl+Shift+F9，编译<br>Ctrl+Shift+F10，运行<br>Ctrl+Shift+F8，查看断点<br>F8，步过<br>F7，步入<br>Shift+F7，智能步入<br>Shift+F8，步出<br>Alt+Shift+F8，强制步过<br>Alt+Shift+F7，强制步入<br>Alt+F9，运行至光标处<br>Ctrl+Alt+F9，强制运行至光标处<br>F9，恢复程序<br>Alt+F10，定位到断点<br>Ctrl+F8，切换行断点<br>Ctrl+F9，生成项目<br>Alt+1，项目<br>Alt+2，收藏<br>Alt+6，TODO<br>Alt+7，结构<br>Ctrl+Shift+C，复制路径<br>Ctrl+Alt+Shift+C，复制引用，必须选择类名<br>Ctrl+Alt+Y，同步<br>Ctrl+~，快速切换方案（界面外观、代码风格、快捷键映射等菜单）<br>Shift+F12，还原默认布局<br>Ctrl+Shift+F12，隐藏/恢复所有窗口<br>Ctrl+F4，关闭<br>Ctrl+Shift+F4，关闭活动选项卡<br>Ctrl+Tab，转到下一个拆分器<br>Ctrl+Shift+Tab，转到上一个拆分器</p>
<h2 id="【重构】"><a href="#【重构】" class="headerlink" title="【重构】"></a>【重构】</h2><p>Ctrl+Alt+Shift+T，弹出重构菜单<br>Shift+F6，重命名<br>F6，移动<br>F5，复制<br>Alt+Delete，安全删除<br>Ctrl+Alt+N，内联</p>
<h2 id="【查找】"><a href="#【查找】" class="headerlink" title="【查找】"></a>【查找】</h2><p>Ctrl+F，查找<br>Ctrl+R，替换<br>F3，查找下一个<br>Shift+F3，查找上一个<br>Ctrl+Shift+F，在路径中查找<br>Ctrl+Shift+R，在路径中替换<br>Ctrl+Shift+S，搜索结构<br>Ctrl+Shift+M，替换结构<br>Alt+F7，查找用法<br>Ctrl+Alt+F7，显示用法<br>Ctrl+F7，在文件中查找用法<br>Ctrl+Shift+F7，在文件中高亮显示用法</p>
]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown基本语法</title>
    <url>/post/8979b4c0/</url>
    <content><![CDATA[<h2 id="一、标题"><a href="#一、标题" class="headerlink" title="一、标题"></a>一、标题</h2><p>在想要设置为标题的文字前面加#来表示<br>一个#是一级标题，二个#是二级标题，以此类推。支持六级标题。</p>
<p>注：标准语法一般在#后跟个空格再写文字，貌似简书不加空格也行。</p>
<p>示例：</p>
<p><code># 这是一级标题</code><br><br><code>## 这是二级标题</code><br><br><code>### 这是三级标题</code><br><br><code>#### 这是四级标题</code><br><br><code>##### 这是五级标题</code><br><br><code>###### 这是六级标题</code><br><br>效果如下：</p>
<h1 id="这是一级标题"><a href="#这是一级标题" class="headerlink" title="这是一级标题"></a>这是一级标题</h1><h2 id="这是二级标题"><a href="#这是二级标题" class="headerlink" title="这是二级标题"></a>这是二级标题</h2><h3 id="这是三级标题"><a href="#这是三级标题" class="headerlink" title="这是三级标题"></a>这是三级标题</h3><h4 id="这是四级标题"><a href="#这是四级标题" class="headerlink" title="这是四级标题"></a>这是四级标题</h4><h5 id="这是五级标题"><a href="#这是五级标题" class="headerlink" title="这是五级标题"></a>这是五级标题</h5><h6 id="这是六级标题"><a href="#这是六级标题" class="headerlink" title="这是六级标题"></a>这是六级标题</h6><h2 id="二、字体"><a href="#二、字体" class="headerlink" title="二、字体"></a>二、字体</h2><p>加粗<br>要加粗的文字左右分别用两个*号包起来</p>
<p>斜体<br>要倾斜的文字左右分别用一个*号包起来</p>
<p>斜体加粗<br>要倾斜和加粗的文字左右分别用三个*号包起来</p>
<p>删除线<br>要加删除线的文字左右分别用两个~~号包起来</p>
<p>示例：</p>
<p><code>**这是加粗的文字**</code><br><br><code>*这是倾斜的文字*``&lt;br&gt;</code><strong><em>这是斜体加粗的文字</em></strong><code>&lt;br&gt;</code><del>这是加删除线的文字</del>`<br><br>效果如下：</p>
<p><strong>这是加粗的文字</strong><br><em>这是倾斜的文字</em><br><strong><em>这是斜体加粗的文字</em></strong><br><del>这是加删除线的文字</del></p>
<h2 id="三、引用"><a href="#三、引用" class="headerlink" title="三、引用"></a>三、引用</h2><p>在引用的文字前加&gt;即可。引用也可以嵌套，如加两个&gt;&gt;三个&gt;&gt;&gt;<br>n个…<br>貌似可以一直加下去，但没神马卵用</p>
<p>示例：</p>
<p><code>&gt;这是引用的内容</code><br><br><code>&gt;&gt;这是引用的内容</code><br><br><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;这是引用的内容</code><br><br>效果如下：</p>
<blockquote>
<p>这是引用的内容</p>
<blockquote>
<p>这是引用的内容</p>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>这是引用的内容</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h2 id="四、分割线"><a href="#四、分割线" class="headerlink" title="四、分割线"></a>四、分割线</h2><p>三个或者三个以上的 - 或者 * 都可以。</p>
<p>示例：</p>
<p><code>---</code><br><br><code>----</code><br><br><code>***</code><br><br><code>*****</code><br><br>效果如下：<br>可以看到，显示效果是一样的。</p>
<hr>
<hr>
<hr>
<hr>
<h2 id="五、图片"><a href="#五、图片" class="headerlink" title="五、图片"></a>五、图片</h2><p>语法：</p>
<p><code>![图片alt](图片地址 &#39;&#39;图片title&#39;&#39;)</code></p>
<p>图片alt就是显示在图片下面的文字，相当于对图片内容的解释。<br>图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加<br>示例：</p>
<p>![blockchain](<a href="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/" target="_blank" rel="noopener">https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/</a><br>u=702257389,1274025419&amp;fm=27&amp;gp=0.jpg “区块链”)<br>效果如下：</p>
<p>blockchain<br>上传本地图片直接点击导航栏的图片标志，选择图片即可</p>
<h2 id="六、超链接"><a href="#六、超链接" class="headerlink" title="六、超链接"></a>六、超链接</h2><p>语法：</p>
<p><a href="超链接地址" title="超链接title">超链接名</a><br>title可加可不加<br>示例：</p>
<p><code>[简书](http://jianshu.com)</code><br><br><code>[百度](http://baidu.com)</code><br><br>效果如下：</p>
<p><a href="http://jianshu.com" target="_blank" rel="noopener">简书</a><br><a href="http://baidu.com" target="_blank" rel="noopener">百度</a></p>
<p>注：Markdown本身语法不支持链接在新页面中打开，貌似简书做了处理，是可以的。别的平台可能就不行了，如果想要在新页面中打开的话可以用html语言的a标签代替。</p>
<p><code>&lt;a href=&quot;超链接地址&quot; target=&quot;_blank&quot;&gt;超链接名&lt;/a&gt;</code></p>
<p>示例<br><a href="https://www.jianshu.com/u/1f5ac0cf6a8b" target="_blank">简书</a></p>
<h2 id="七、列表"><a href="#七、列表" class="headerlink" title="七、列表"></a>七、列表</h2><p>无序列表</p>
<h4 id="语法：无序列表用-任何一种都可以"><a href="#语法：无序列表用-任何一种都可以" class="headerlink" title="语法：无序列表用 - + * 任何一种都可以"></a>语法：无序列表用 - + * 任何一种都可以</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- 列表内容</span><br><span class="line">+ 列表内容</span><br><span class="line">* 列表内容</span><br></pre></td></tr></table></figure>
<p>注意：- + * 跟内容之间都要有一个空格</p>
<p>效果如下：</p>
<ul>
<li>列表内容</li>
</ul>
<ul>
<li>列表内容</li>
</ul>
<ul>
<li>列表内容</li>
</ul>
<p>语法：数字加点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.列表内容</span><br><span class="line">2.列表内容</span><br><span class="line">3.列表内容</span><br></pre></td></tr></table></figure>
<p>注意：序号跟内容之间要有空格<br>效果如下：</p>
<p>1.列表内容<br>2.列表内容<br>3.列表内容</p>
<p>列表嵌套<br>上一级和下一级之间敲三个空格即可</p>
<p>一级无序列表内容<br>   二级无序列表内容<br>   二级无序列表内容<br>   二级无序列表内容<br>一级无序列表内容<br>   二级有序列表内容<br>   二级有序列表内容<br>   二级有序列表内容<br>一级有序列表内容<br>   二级无序列表内容<br>   二级无序列表内容<br>   二级无序列表内容<br>一级有序列表内容<br>   二级有序列表内容<br>   二级有序列表内容<br>   二级有序列表内容</p>
<h2 id="八、表格"><a href="#八、表格" class="headerlink" title="八、表格"></a>八、表格</h2><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">表头|表头|表头</span><br><span class="line">---|:--:|---:</span><br><span class="line">内容|内容|内容</span><br><span class="line">内容|内容|内容</span><br></pre></td></tr></table></figure>
<p>第二行分割表头和内容。</p>
<ul>
<li>有一个就行，为了对齐，多加了几个<br>文字默认居左</li>
<li>两边加：表示文字居中</li>
<li>右边加：表示文字居右<br>注：原生的语法两边都要用 | 包起来。此处省略<br>示例：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">姓名|技能|排行</span><br><span class="line">--|:--:|--:</span><br><span class="line">刘备|哭|大哥</span><br><span class="line">关羽|打|二哥</span><br><span class="line">张飞|骂|三弟</span><br></pre></td></tr></table></figure>
效果如下：</li>
</ul>
<table>
<thead>
<tr>
<th>姓名</th>
<th align="center">技能</th>
<th align="right">排行</th>
</tr>
</thead>
<tbody><tr>
<td>刘备</td>
<td align="center">哭</td>
<td align="right">大哥</td>
</tr>
<tr>
<td>关羽</td>
<td align="center">打</td>
<td align="right">二哥</td>
</tr>
<tr>
<td>张飞</td>
<td align="center">骂</td>
<td align="right">三弟</td>
</tr>
<tr>
<td>## 九、代码</td>
<td align="center"></td>
<td align="right"></td>
</tr>
<tr>
<td>语法：</td>
<td align="center"></td>
<td align="right"></td>
</tr>
<tr>
<td>单行代码：代码之间分别用一个反引号包起来</td>
<td align="center"></td>
<td align="right"></td>
</tr>
</tbody></table>
<pre><code>`代码内容`</code></pre><p>代码块：代码之间分别用三个反引号包起来，且两边的反引号单独占一行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(```)---去掉括号</span><br><span class="line"> 代码...</span><br><span class="line"> 代码...</span><br><span class="line"> 代码...</span><br><span class="line">(```)---去掉括号</span><br></pre></td></tr></table></figure>
<p>注：为了防止转译，前后三个反引号处加了小括号，实际是没有的。这里只是用来演示，实际中去掉两边小括号即可。</p>
<p>示例：</p>
<p>单行代码</p>
<p><code>create database hero;</code><br>代码块</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function fun()&#123;</span><br><span class="line">     echo &quot;这是一句非常牛逼的代码&quot;;</span><br><span class="line">&#125;</span><br><span class="line">fun();</span><br></pre></td></tr></table></figure>
<p>效果如下：</p>
<p>单行代码</p>
<p><code>create database hero;</code></p>
<p>代码块</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function fun()&#123;</span><br><span class="line">  echo &quot;这是一句非常牛逼的代码&quot;;</span><br><span class="line">&#125;</span><br><span class="line">fun();</span><br></pre></td></tr></table></figure>
<h2 id="十、流程图"><a href="#十、流程图" class="headerlink" title="十、流程图"></a>十、流程图</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">st=&gt;start: 开始</span><br><span class="line">op=&gt;operation: My Operation</span><br><span class="line">cond=&gt;condition: Yes or No?</span><br><span class="line">e=&gt;end</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br><span class="line">&amp;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
  </entry>
</search>
